---
title: "Floor prediction By XGBoost"
output: html_notebook
---

#Introduction

Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselve.

There are three different task done by the Machine Learning Algorithms

1) Classifiaction.
2) Clustering.
3) Regression.

This article is confined to classification.

#What is classification ?
This is the problem of identifing to which set of categories my new observation Belongs.The goal of classification is to find Boundaries that best separate different categories of data.

#Type of classification.
There are two type of classification.

1)Binary Classification.
This type of classification is done when we have only 2 class to classify.

2)Multi-class classification.
This type of classification is done when we have more than 2 class to classify.

Today we are going to deal with Multi-class classification problem by XGBoost.

#problem Statment.
There is a device which has collected the data ,now the device needs to dedicate any new floor by the help of previous data.

#Solution
we are going to use Machine Learning Algorithms and build a prediction model for this device which can predict on which floor it's standing by collecting the properties of the floor.


library 's used.
```{r}
library(data.table)
library(ggplot2)
library(plotly)
library(dplyr)
library(corrplot)
library(kableExtra)
library(mltools)
library(caTools)
library(xgboost)
library(Ckmeans.1d.dp) 
library(Matrix)
library(readr)
library(car)
library(caret)
library(lattice)
#The package for random forest function
library(MASS)
library(randomForest)
#correlation plot
library(corrplot)
```

Importing all the data set into R.we are dropping the row_id, group_id as all the variables in this column are unique,so this does not come handy for our analysis. 
```{r}
#Reading the predictors.
x_train<-fread("X_train.csv",stringsAsFactors = T,drop = "row_id")
#Reading the target variable.
y_train<-fread("Y_train.csv",stringsAsFactors = T,drop = "group_id")
#Reading the testing data
test<-fread("X_test.csv",stringsAsFactors = T,drop = "row_id")
```
Viewing the structure of the both training and testing data.
```{r}
str(x_train)
str(y_train)
```
Now we are going to join x_train and y_train by series_id.
```{r}
train_data<-merge(x_train,y_train,by="series_id")
```

Seing the columns names.
```{r}
colnames(train_data)
```
so these are our feature where "Surface"" is our target feature and all others act as predictors in our model building.

Lets check how many types of floor do we have.
```{r}
table(train_data$surface)
```
so we have 9 Types of floor.

so we need to do multiclass classification.

#Data Cleaning

Summary of the data.
```{r}
summary(train_data)
```


Now checking the NA 's and Empty spaces  in the data.
```{r}
cat("\nThe Total number of NA 's in the train data is :- ",sum(is.na(train_data)))
cat("\nThe Total number of empty spaces 's in the train data is :- ",sum(train_data==" "))
```
Checking the percentage of each class in the surface feature(Target Variable),checking the  ImmBalanced class.
```{r}
#Donet chart which display the percentage of each class in the column
 p <- train_data %>%
  group_by(surface) %>%
  summarize(count = n()) %>%
  plot_ly(labels = ~surface, values = ~count) %>%
  add_pie(hole = 0.6) %>%
  layout(title = "The Percentage of category at Surface column",  showlegend = F,
         xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
         yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
```
As each categories are distributed uniformly ,so there is no problem of miss classification.

Encoding the target variable.
```{r}
train_data$surface <-as.numeric(as.factor(train_data$surface))-1
```

Actual Values                                   Encoded Values

carpet                                              0
concrete 99712                                      1
fine_concrete 46464                                 2
hard_tiles  2688                                    3
hard_tiles_large_space 39424                        4
soft_pvc 93696                                      5
soft_tiles 38016                                    6
tiled 65792                                         7
wood 7                                              8

```{r}
corr<-cor(x_train)
corrplot(corr,type = "lower")
```
There are high positive correlation between the predictor variable orientation_w with orientation_x,orientation_z with orientation_y and there is high negative correlation between the predictor variables angular_velocity_y and angular_velocity_z.

As we are using XGBoost for our model building so we does not  need to handle  multicollinearity as XGBoost can handle it by it self. 


#The XGBoost Model Building

Dividing the data set into train test.
```{r}
set.seed(1)
## 75% of the sample size
smp_size <- floor(0.75 * nrow(train_data))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(train_data)), size = smp_size)

train_train_data <- train_data[train_ind,]
test_train_data <- train_data[-train_ind,]
```

Now checking the classs imbalance in the training data.
```{r}
#Donet chart which display the percentage of each class in the column
 p <- train_train_data %>%
  group_by(surface) %>%
  summarize(count = n()) %>%
  plot_ly(labels = ~surface, values = ~count) %>%
  add_pie(hole = 0.6) %>%
  layout(title = "The Percentage of category at Surface column",  showlegend = F,
         xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
         yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
```
As we see all the categorical variables are properly balanced ,so now we use this train_train_data for Model Building.

Now applying XGBoost algorithm for classification.

preparing the data for xgboost algorithm.
note :- XGBoost can not handle categorical or numeric data.
we should always pass the data in for of matrix for xgboost.
```{r}
dtrain <- xgb.DMatrix(as.matrix(train_train_data[,-"surface"]), label = as.matrix(train_train_data$surface))
dtestfinal<-xgb.DMatrix(as.matrix(test_train_data[,-"surface"]), label = as.matrix(test_train_data$surface))
```

Setting parameters for xgboost
```{r}
#default parameters
params <- list(booster = "gbtree",num_class=9 ,objective = "multi:softmax", eta=0.2, max_depth=4, min_child_weight=2, subsample=1, colsample_bytree=1)
```

Doing parameter tuning
```{r}
#find best nround
cv<-xgb.cv( params = params, data = dtrain, nrounds = 50, nfold = 5,gamma=0, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)
cv$best_iteration
print(cv)
print(cv, verbose=TRUE)
```
As we in the above output we got minimum error iteration 50.

Now training the model on train data set.
```{r}
#first default - model training
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 50, watchlist = list(val=dtestfinal,train=dtrain), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = "mlogloss")
```
Now we are using our model on test data set.
```{r}
#model prediction
xgbpred <- predict (xgb1,dtestfinal)
```

#Confusion Matrix
checking the results of the model.
```{r}
result<-confusionMatrix(table(as.factor(test_train_data$surface), as.factor(xgbpred)),mode = "prec_recall")
result
```
Our Model has given accuracy of 88%.


The important features given by XGBoost 
```{r}
xgb.importance( model =xgb1)
```
As we see above the orientation_x is the most important feature for predicting the type of floor.

Now we are going to use our model for predicting the test data.
preprocessing the test data.
```{r}
dtestfinal<-xgb.DMatrix(as.matrix(test))
```


#Using the model for Test Data
predicting the values for the Test data .
```{r}
predicted_value<-predict (xgb1,dtestfinal)
class(predicted_value)
```
Function for decoding the variables. 
```{r}
Decoding<-function(x){
  if(x==0){
    return("carpet")
  }else if(x==1){
    return("concrete")
  }else if(x==2){
    return("fine_concrete")
  }else if(x==3){
    return("hard_tiles")
  }else if(x==4){
    return("hard_tiles_large_space ")
  }else if(x==5){
    return("soft_pvc")
  }else if(x==6){
    return("soft_tiles")
  }else if(x==7){
    return("tiled")
  }else{
    return("wood")
  }
}

```

carpet                                              0
concrete                                            1
fine_concrete                                       2
hard_tiles                                          3
hard_tiles_large_space                              4
soft_pvc                                            5
soft_tiles                                          6
tiled                                               7
wood                                                8

Now we are doing Decoding our target variable to their respective categories given above .
```{r}
predicted_value<-sapply(predicted_value,Decoding)
```

so Final predictions are :-
```{r}
test$surface<-as.factor(predicted_value)
```

Viewing the test data after predictions.

#The OutPut.
This is the following expected output.
```{r}
test[,c("series_id","surface")]
```

