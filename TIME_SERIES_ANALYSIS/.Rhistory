#for unit
library(tidyr)
unite(air_visit_data,id,c("air_store_id","visit_date"))
View(data)
library(data.table)
#for unit
library(tidyr)
#library()
air_reserve<-fread("air_reserve.csv")
air_store_info<-fread("air_store_info.csv")
air_visit_data<-fread("air_visit_data.csv")
data_info<-fread("date_info.csv")
hpg_reserve<-fread("hpg_reserve.csv")
hpg_store_info<-fread("hpg_store_info.csv")
store_id_relation<-fread("store_id_relation.csv")
library(data.table)
#for unit
library(tidyr)
#library()
air_reserve<-fread("air_reserve.csv")
air_store_info<-fread("air_store_info.csv")
air_visit_data<-fread("air_visit_data.csv")
data_info<-fread("date_info.csv")
hpg_reserve<-fread("hpg_reserve.csv")
hpg_store_info<-fread("hpg_store_info.csv")
store_id_relation<-fread("store_id_relation.csv")
data_buffer<-merge(air_reserve,air_store_info,by="air_store_id",all=TRUE)
data<-merge(data_buffer,store_id_relation,by="air_store_id",all=TRUE)
air_visit_data
install.packages(“twitteR”)
install.packages(“twitteR”)
install.packages(twitteR)
install.packages("twitteR")
library("twitteR")
#load credentials
consumer_key<-"p0f5zNiHE476kdUPHF2cGXyLM"
consumer_secret<-"bTtTNK8s0oAIUaJrmb7JNUqGFuMY3ekZJ20UdmoZRe5F1DDwgO"
access_token<- "817441188466671616-8m3FPiGo3T6ILUcFhdgLSLxXg7kT3ol"
access_secret<- "a5wg4JaREyFXigSwc6jkfooICG8BZLkltmqEaqUU69scT"
#set up to authenticate
setup_twitter_oauth(consumer_key ,consumer_secret,access_token ,access_secret)
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <- twitteR::searchTwitter(“#MachineLearning”,n =12,lang =”en”,since = ‘2018–01–01’)
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <- twitteR::searchTwitter("#MachineLearning”,n =12,lang =”en”,since = ‘2018–01–01’)
#strip retweets
strip_retweets(tweets)
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <- twitteR::searchTwitter("#Machine Learning”,n =1000,lang =”en”,since = ‘2018–01–01’)
#strip retweets
strip_retweets(tweets)
library("twitteR")
#load credentials
consumer_key<-"p0f5zNiHE476kdUPHF2cGXyLM"
consumer_secret<-"bTtTNK8s0oAIUaJrmb7JNUqGFuMY3ekZJ20UdmoZRe5F1DDwgO"
access_token<- "817441188466671616-8m3FPiGo3T6ILUcFhdgLSLxXg7kT3ol"
access_secret<- "a5wg4JaREyFXigSwc6jkfooICG8BZLkltmqEaqUU69scT"
#set up to authenticate
setup_twitter_oauth(consumer_key ,consumer_secret,access_token ,access_secret)
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <- twitteR::searchTwitter("#Machine Learning”,n =10000,lang =”en”,since = ‘2018–01–01’)
#strip retweets
strip_retweets(tweets)
library("twitteR")
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <- twitteR::searchTwitter("#Machine Learning”,n =10000,lang =”en”,since = ‘2018–01–01’)
#strip retweets
strip_retweets(tweets)
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <- twitteR::searchTwitter('#Machine Learning',n =10000,lang ="en”,since = '2018–01–01')
#strip retweets
strip_retweets(tweets)
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets<-twitteR::searchTwitter('#Machine Learning',n =10000,lang ="en”,since = '2018–01–01')
#strip retweets
strip_retweets(tweets)
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets<-twitteR::searchTwitter('#Machine Learning',n =10000,lang ="en”,since = '2018–01–01')
#strip retweets
strip_retweets(tweets)
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets<-twitteR::searchTwitter("#Machine Learning",n =10000,lang ="en”,since = '2018–01–01")
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets<-twitteR::searchTwitter("#Machine Learning",n =10000,lang ="en”,since = '2018–01–01")
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <- twitteR::searchTwitter("#Machine Learning" ,n =12,lang ="en",since = ‘2018–01–01’)
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <- twitteR::searchTwitter("#Machine Learning" , n =12 ,lang = "en",since = "2018–01–01")
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <- twitteR::searchTwitter("#Machine Learning" , n =12 ,lang = "en",since = '2018–01–01')
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <- twitteR::searchTwitter("#Machine Learning",n =12,lang ="en",since ='2018–01–01')
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <- twitteR::searchTwitter("#Machine Learning",n =12,lang ="en",since =as.Date('2018–01–01'))
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <- twitteR::searchTwitter("#Machine Learning",n =10000,lang ="en")
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <-as.data.table(twitteR::searchTwitter("#Machine Learning",n =10000,lang ="en"))
View(tweets)
View(tweets)
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <-as.data.table(twitteR::searchTwitter("#Machine Learning",n =10000,lang ="en",since='2018-01-01'))
#fetch tweets associated with that hashtag , 12 tweets-n in
#(en)glish-lang since the indicated date yy/mm/dd
tweets <-twitteR::searchTwitter("#Machine Learning",n =10000,lang ="en",since='2018-01-01')
#strip retweets
strip_retweets(tweets)
#convert to data frame using the twListtoDF function
df <- twListToDF(tweets)\#extract the data frame save it locally
#convert to data frame using the twListtoDF function
df <- twListToDF(tweets)#extract the data frame save it locally
saveRDS(df, file=”tweets.rds”)
#convert to data frame using the twListtoDF function
df <- twListToDF(tweets)#extract the data frame save it locally
saveRDS(df, file= "tweets.rds")
df1 <- readRDS("mytweets.rds")
#convert to data frame using the twListtoDF function
df <- twListToDF(tweets)#extract the data frame save it locally
saveRDS(df, file= "tweets.rds")
#convert all text to lower case
df_lower<- tolower(df$text)
# Replace blank space (“rt”)
df_lower <- gsub("rt", "", df_lower)
# Replace @UserName
df_lower <- gsub("@\\w+", "", df_lower)
# Remove punctuation
df_lower <- gsub("[[:punct:]]", "", df_lower)
# Remove links
df_lower <- gsub("http\\w+", "", df_lower)
# Remove tabs
df_lower<- gsub("[ |\t]{2,}", "", df_lower)
# Remove blank spaces at the beginning
df_lower <- gsub("^ ", "", df_lower)
# Remove blank spaces at the end
df_lower <- gsub(" $", "",df_lower )
#clean up by removing stop words
df_lower <- tm_map(df_lower, function(x)removeWords(x,stopwords()))
?tm_map
install.packages("ROAuth")
library("twitteR")
install.packages("ROAuth")
library("NLP", lib.loc="~/R/win-library/3.3")
#install.packages("ROAuth")
library(ROAuth)
install.packages("NLP")
library(NLP)
library("NLP", lib.loc="~/R/win-library/3.3")
install.packages("syuzhet")
#install.packages("syuzhet")
library("syuzhet", lib.loc="~/R/win-library/3.3")
#install.packages("syuzhet")
library("syuzhet", lib.loc="~/R/win-library/3.3")
install.packages("syuzhet")
library("syuzhet", lib.loc="~/R/win-library/3.3")
library("syuzhet")
install.packages("tm")
#install.packages("tm")
library("tm", lib.loc="~/R/win-library/3.3")
#install.packages("NLP")
library("NLP", lib.loc="~/R/win-library/3.3")
#install.packages("tm")
library("tm", lib.loc="~/R/win-library/3.3")
#install.packages("tm")
library("tm")
install.packages("SnowballC")
#install.packages("SnowballC")
library("SnowballC", lib.loc="~/R/win-library/3.3")
#install.packages("SnowballC")
library("SnowballC")
install.packages("stringi")
install.packages("stringi")
#install.packages("stringi")
library("stringi", lib.loc="~/R/win-library/3.3")
#install.packages("stringi")
library("stringi")
install.packages("stringi")
library("stringi")
install.packages("topicmodels")
library("topicmodels")
install.packages("syuzhet")
library("syuzhet")
install.packages("ROAuth")
library("twitteR")
#install.packages("ROAuth")
library(ROAuth)
#install.packages("NLP")
library("NLP", lib.loc="~/R/win-library/3.3")
library("twitteR")
#install.packages("ROAuth")
library(ROAuth)
#install.packages("NLP")
library("NLP")
library("twitteR")
#install.packages("syuzhet")
library("syuzhet")
#install.packages("tm")
library("tm")
#install.packages("SnowballC")
library("SnowballC")
#install.packages("stringi")
library("stringi")
#install.packages("topicmodels")
library("topicmodels")
#install.packages("syuzhet")
library("syuzhet")
#install.packages("ROAuth")
library("ROAuth")
#clean up by removing stop words
df_lower <- tm_map(df_lower, function(x)removeWords(x,stopwords()))
#clean up by removing stop words
df_lower <- tm_map(as.character(df_lower), function(x)removeWords(x,stopwords()))
View(df)
#clean up by removing stop words
corpus<-Corpus(VectorSource(df_lower))
corpus <- tm_map(corpus, function(x)removeWords(x,stopwords()))
library("wordcloud")
install.packages("wordcloud")
library("wordcloud")
#generate wordcloud
wordcloud(corpus,min.freq = 10,colors=brewer.pal(8, "Dark2"),random.color = TRUE,max.words = 500)
#getting emotions using in-built function
mysentiment_google<-get_nrc_sentiment((df_lower))
#getting emotions using in-built function
mysentiment<-get_nrc_sentiment((df_lower))
#calculationg total score for each sentiment
Sentimentscores<-data.frame(colSums(mysentiment[,]))
names(Sentimentscores)<-"Score"
Sentimentscores<-cbind("sentiment"=rownames(Sentimentscores),Sentimentscores)
rownames(Sentimentscores)<-NULL
#plotting the sentiments with scores
ggplot(data=Sentimentscores,aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people behind the tweets on tech giant GOOGLE")
library(ggplot2)
#plotting the sentiments with scores
ggplotly(ggplot(data=Sentimentscores,aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people behind the tweets on tech giant GOOGLE"))
library(ggplotly)
library(plotly)
#plotting the sentiments with scores
ggplotly(ggplot(data=Sentimentscores,aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people behind the tweets on tech giant GOOGLE"))
#plotting the sentiments with scores
ggplotly(ggplot(data=Sentimentscores,aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people behind the tweets on tech giant GOOGLE"))
#Donet chart which display the percentage of each class in the column
p <- Sentimentscores %>%
group_by(sentiment) %>%
summarize(count = n()) %>%
plot_ly(labels = ~sentiment, values = ~Score) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of Sentiments of people behind the tweets on #MachineLearning",  showlegend = F,
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
#Donet chart which display the percentage of each class in the column
p <- Sentimentscores %>%
group_by(sentiment) %>%
summarize(count = n()) %>%
plot_ly(labels = ~sentiment, values = ~Score) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of Sentiments of people behind the tweets on #MachineLearning",  showlegend = F,
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
library("twitteR")
#install.packages("ROAuth")
library(ROAuth)
#install.packages("NLP")
library("NLP")
library("twitteR")
#install.packages("syuzhet")
library("syuzhet")
#install.packages("tm")
library("tm")
#install.packages("SnowballC")
library("SnowballC")
#install.packages("stringi")
library("stringi")
#install.packages("topicmodels")
library("topicmodels")
#install.packages("syuzhet")
library("syuzhet")
#install.packages("ROAuth")
library("ROAuth")
install.packages("wordcloud")
library("wordcloud")
library(ggplot2)
library(plotly)
library(dplyr)
install.packages("wordcloud")
library("twitteR")
#install.packages("ROAuth")
library(ROAuth)
#install.packages("NLP")
library("NLP")
library("twitteR")
#install.packages("syuzhet")
library("syuzhet")
#install.packages("tm")
library("tm")
#install.packages("SnowballC")
library("SnowballC")
#install.packages("stringi")
library("stringi")
#install.packages("topicmodels")
library("topicmodels")
#install.packages("syuzhet")
library("syuzhet")
#install.packages("ROAuth")
library("ROAuth")
install.packages("wordcloud")
library("wordcloud")
library(ggplot2)
library(plotly)
library(dplyr)
#Donet chart which display the percentage of each class in the column
p <- Sentimentscores %>%
group_by(sentiment) %>%
summarize(count = n()) %>%
plot_ly(labels = ~sentiment, values = ~Score) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of Sentiments of people behind the tweets on #MachineLearning",  showlegend = F,
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
#Donet chart which display the percentage of each class in the column
p <- Sentimentscores %>%
group_by(sentiment) %>%
summarize(count = n()) %>%
plot_ly(labels = ~sentiment, values = ~Sentimentscores$Score) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of Sentiments of people behind the tweets on #MachineLearning",  showlegend = F,
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
#plotting the sentiments with scores
ggplotly(ggplot(data=Sentimentscores,aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people behind the tweets on #MachineLearning"))
#Donet chart which display the percentage of each class in the column
p <- Sentimentscores %>%
group_by(sentiment) %>%
summarize(count = n()) %>%
plot_ly(labels = ~sentiment, values = Sentimentscores$Score) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of Sentiments of people behind the tweets on #MachineLearning",  showlegend = F,
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
#Donet chart which display the percentage of each class in the column
p <- Sentimentscores %>%
group_by(sentiment) %>%
summarize(count = n()) %>%
plot_ly(labels = sentiment, values = Sentimentscores$Score) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of Sentiments of people behind the tweets on #MachineLearning",  showlegend = F,
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
#Donet chart which display the percentage of each class in the column
p <- Sentimentscores %>%
group_by(sentiment) %>%
summarize(count = n()) %>%
plot_ly(labels = ~sentiment, values = Sentimentscores$Score) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of Sentiments of people behind the tweets on #MachineLearning",  showlegend = F,
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
#plotting the sentiments with scores
ggplotly(ggplot(data=Sentimentscores,aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people behind the tweets on #MachineLearning")+flip())
#plotting the sentiments with scores
ggplotly(ggplot(data=Sentimentscores,aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people behind the tweets on #MachineLearning")+coord_flip())
#generate wordcloud
wordcloud(corpus,min.freq = 10,colors=brewer.pal(8, "Dark2"),random.color = TRUE,max.words = 500)
#generate wordcloud
wordcloud(corpus,min.freq = 10,colors=brewer.pal(8, "Dark2"),random.color = TRUE,max.words = 500)
#Donet chart which display the percentage of each class in the column
# p <- Sentimentscores %>%
# group_by(sentiment) %>%
#  summarize(count = n()) %>%
#  plot_ly(labels = ~sentiment, values = Sentimentscores$Score) %>%
#  add_pie(hole = 0.6) %>%
#  layout(title = "The Percentage of Sentiments of people behind the tweets on #MachineLearning",  showlegend = F,
#         yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
#p
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 20)
library(data.table)
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d<-as.data.table(d)
head(d, 20)
d<- d[with(d,order(-freq)),]
head(d, 20)
ggplotly(d[1:10,],aes(x=reorder(word,freq),y=freq))+geom_bar(stat = "identity")
ggplotly(d[1:10,],aes(x=reorder(word,freq),y=freq))+geom_bar(stat = "identity"))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq))+geom_bar(stat = "identity"))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=0))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=-1))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=-20))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=20))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq,fill=as.factor(word)))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq,fill=as.factor(word)))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10)+labs(title = "The Top 10 frequently used words",x="Words", y = "Frequency of Words",fill="Words")+theme(plot.title = element_text(hjust = 0.5,face = "bold",color = "black")))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq,fill=as.factor(word)))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10)+labs(title = "The Top 10 frequently used words",x="Words", y = "Frequency of Words",fill="Words")+theme(plot.title = element_text(hjust = 0.5,face = "bold",color = "black"))+ylim(0,1300))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq,fill=as.factor(word)))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10)+labs(title = "The Top 10 frequently used words",x="Words", y = "Frequency of Words",fill="Words")+theme(plot.title = element_text(hjust = 2,face = "bold",color = "black"))+ylim(0,1300))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq,fill=as.factor(word)))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10)+labs(title = "The Top 10 frequently used words",x="Words", y = "Frequency of Words",fill="Words")+theme(plot.title = element_text(hjust = -0.5,face = "bold",color = "black"))+ylim(0,1300))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq,fill=as.factor(word)))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10)+labs(title = "The Top 10 frequently used words",x="Words", y = "Frequency of Words",fill="Words")+theme(plot.title = element_text(hjust = -20,face = "bold",color = "black"))+ylim(0,1300))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq,fill=as.factor(word)))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10)+labs(title = "The Top 10 frequently used words",x="Words", y = "Frequency of Words",fill="Words")+theme(plot.title = element_text(hjust = -10,face = "bold",color = "black"))+ylim(0,1300))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq,fill=as.factor(word)))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10)+labs(title = "The Top 10 frequently used words",x="Words", y = "Frequency of Words",fill="Words")+theme(plot.title = element_text(hjust = -10,face = "bold",color = "black")))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq,fill=as.factor(word)))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10)+labs(title = "The Top 10 frequently used words",x="Words", y = "Frequency of Words",fill="Words")+theme(plot.title = element_text(hjust = -10,face = "bold",color = "black")))
#plotting the sentiments with scores
ggplotly(ggplot(data=Sentimentscores,aes(x=reorder(sentiment,Score),y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")++ggtitle("Sentiments of people behind the tweets on #MachineLearning")+coord_flip())
#plotting the sentiments with scores
ggplotly(ggplot(data=Sentimentscores,aes(x=reorder(sentiment,Score),y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")++ggtitle("Sentiments of people behind the tweets on #MachineLearning")+coord_flip())
#plotting the sentiments with scores
ggplotly(ggplot(data=Sentimentscores,aes(x=reorder(sentiment,Score),y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people behind the tweets on #MachineLearning")+coord_flip())
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq,fill=as.factor(word)))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10)+labs(title = "The Top 10 frequently used words",x="Words", y = "Frequency of Words",fill="Words")+theme(plot.title = element_text(hjust = -10,face = "bold",color = "black"))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq,fill=as.factor(word)))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10)+labs(title = "The Top 10 frequently used words",x="Words", y = "Frequency of Words",fill="Words")+theme(plot.title = element_text(hjust = -10,face = "bold",color = "black")))
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq,fill=as.factor(word)))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10)+labs(title = "The Top 10 frequently used words",x="Words", y = "Frequency of Words",fill="Words")+theme(plot.title = element_text(hjust = -10,face = "bold",color = "black"))+coord_flip())
#plotting the sentiments with scores
ggplotly(ggplot(data=Sentimentscores,aes(x=reorder(sentiment,Score),y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people behind the tweets on #MachineLearning")+labs(title = "Sentiments of people behind the tweets on #MachineLearning",x="Sentiments", y = "Scores")+theme(plot.title = element_text(hjust = -10,face = "bold",color = "black"))+coord_flip())
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
summarize(count = n()) %>%
plot_ly(labels = ~word, values = freq) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of Sentiments of people behind the tweets on #MachineLearning",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
summarize(count = n()) %>%
plot_ly(labels = ~word, values = d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of Sentiments of people behind the tweets on #MachineLearning",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq,fill=as.factor(word)))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10)+labs(title = "The Top 10 frequently used words",x="Words", y = "Frequency of Words",fill="Words")+theme(plot.title = element_text(hjust = -10,face = "bold",color = "black"))+coord_flip())
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
summarize(count = n()) %>%
plot_ly(labels = ~word, values = count) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of Sentiments of people behind the tweets on #MachineLearning",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
summarize(count = n()) %>%
plot_ly(labels = ~word, values =~freq) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of Sentiments of people behind the tweets on #MachineLearning",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
summarize(count = n()) %>%
plot_ly(labels = ~word, values = ~freq) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of Sentiments of people behind the tweets on #MachineLearning",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of Sentiments of people behind the tweets on #MachineLearning",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
