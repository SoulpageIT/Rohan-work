{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# practice recommendation engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries used.\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a practical introduction to the main Recommender System (RecSys) techniques. The objective of a RecSys is to recommend relevant items for users, based on their preference. Preference and relevance are subjective, and they are generally inferred by items users have consumed previously.\n",
    "The main RecSys techniques are:\n",
    "\n",
    "# Collaborative Filtering: \n",
    "This method makes automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on a set of items, A is more likely to have B's opinion for a given item than that of a randomly chosen person.\n",
    "\n",
    "# Content-Based Filtering: \n",
    "This method uses only information about the description and attributes of the items users has previously consumed to model user's preferences. In other words, these algorithms try to recommend items that are similar to those that a user liked in the past (or is examining in the present). In particular, various candidate items are compared with items previously rated by the user and the best-matching items are recommended.\n",
    "\n",
    "# Hybrid methods:\n",
    "\n",
    "Recent research has demonstrated that a hybrid approach, combining collaborative filtering and content-based filtering could be more effective than pure approaches in some cases. These methods can also be used to overcome some of the common problems in recommender systems such as cold start and the sparsity problem.\n",
    "In this notebook, we use a dataset we've shared on Kaggle Datasets: Articles Sharing and Reading from CI&T Deskdrop.\n",
    "We will demonstrate how to implement Collaborative Filtering, Content-Based Filtering and Hybrid methods in Python, for the task of providing personalized recommendations to the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data: CI&T Deskdrop dataset¶\n",
    "In this section, we load the Deskdrop dataset, which contains a real sample of 12 months logs (Mar. 2016 - Feb. 2017) from CI&T's Internal Communication platform (DeskDrop). It contains about 73k logged users interactions on more than 3k public articles shared in the platform. It is composed of two CSV files:\n",
    "\n",
    "shared_articles.csv\n",
    "users_interactions.csv\n",
    "Take a look in this kernels for a better picture of the dataset:\n",
    "\n",
    "Deskdrop datasets EDA\n",
    "DeskDrop Articles Topic Modeling\n",
    "shared_articles.csv\n",
    "Contains information about the articles shared in the platform. Each article has its sharing date (timestamp), the original url, title, content in plain text, the article' lang (Portuguese: pt or English: en) and information about the user who shared the article (author).\n",
    "\n",
    "There are two possible event types at a given timestamp:\n",
    "\n",
    "CONTENT SHARED: The article was shared in the platform and is available for users.\n",
    "CONTENT REMOVED: The article was removed from the platform and not available for further recommendation.\n",
    "For the sake of simplicity, we only consider here the \"CONTENT SHARED\" event type, assuming (naively) that all articles were available during the whole one year period. For a more precise evaluation (and higher accuracy), only articles that were available at a given time should be recommended, but we let this exercice for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>eventType</th>\n",
       "      <th>contentId</th>\n",
       "      <th>authorPersonId</th>\n",
       "      <th>authorSessionId</th>\n",
       "      <th>authorUserAgent</th>\n",
       "      <th>authorRegion</th>\n",
       "      <th>authorCountry</th>\n",
       "      <th>contentType</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1459193988</td>\n",
       "      <td>CONTENT SHARED</td>\n",
       "      <td>-4110354420726924665</td>\n",
       "      <td>4340306774493623681</td>\n",
       "      <td>8940341205206233829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HTML</td>\n",
       "      <td>http://www.nytimes.com/2016/03/28/business/dea...</td>\n",
       "      <td>Ethereum, a Virtual Currency, Enables Transact...</td>\n",
       "      <td>All of this work is still very early. The firs...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1459194146</td>\n",
       "      <td>CONTENT SHARED</td>\n",
       "      <td>-7292285110016212249</td>\n",
       "      <td>4340306774493623681</td>\n",
       "      <td>8940341205206233829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HTML</td>\n",
       "      <td>http://cointelegraph.com/news/bitcoin-future-w...</td>\n",
       "      <td>Bitcoin Future: When GBPcoin of Branson Wins O...</td>\n",
       "      <td>The alarm clock wakes me at 8:00 with stream o...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp       eventType            contentId       authorPersonId  \\\n",
       "1  1459193988  CONTENT SHARED -4110354420726924665  4340306774493623681   \n",
       "2  1459194146  CONTENT SHARED -7292285110016212249  4340306774493623681   \n",
       "\n",
       "       authorSessionId authorUserAgent authorRegion authorCountry contentType  \\\n",
       "1  8940341205206233829             NaN          NaN           NaN        HTML   \n",
       "2  8940341205206233829             NaN          NaN           NaN        HTML   \n",
       "\n",
       "                                                 url  \\\n",
       "1  http://www.nytimes.com/2016/03/28/business/dea...   \n",
       "2  http://cointelegraph.com/news/bitcoin-future-w...   \n",
       "\n",
       "                                               title  \\\n",
       "1  Ethereum, a Virtual Currency, Enables Transact...   \n",
       "2  Bitcoin Future: When GBPcoin of Branson Wins O...   \n",
       "\n",
       "                                                text lang  \n",
       "1  All of this work is still very early. The firs...   en  \n",
       "2  The alarm clock wakes me at 8:00 with stream o...   en  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df = pd.read_csv('shared_articles.csv')\n",
    "articles_df = articles_df[articles_df['eventType'] == 'CONTENT SHARED']\n",
    "articles_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# users_interactions.csv¶\n",
    "Contains logs of user interactions on shared articles. It can be joined to articles_shared.csv by contentId column.\n",
    "\n",
    "The eventType values are:\n",
    "\n",
    "VIEW: The user has opened the article.\n",
    "LIKE: The user has liked the article.\n",
    "COMMENT CREATED: The user created a comment in the article.\n",
    "FOLLOW: The user chose to be notified on any new comment in the article.\n",
    "BOOKMARK: The user has bookmarked the article for easy return in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>eventType</th>\n",
       "      <th>contentId</th>\n",
       "      <th>personId</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userRegion</th>\n",
       "      <th>userCountry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1465413032</td>\n",
       "      <td>VIEW</td>\n",
       "      <td>-3499919498720038879</td>\n",
       "      <td>-8845298781299428018</td>\n",
       "      <td>1264196770339959068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1465412560</td>\n",
       "      <td>VIEW</td>\n",
       "      <td>8890720798209849691</td>\n",
       "      <td>-1032019229384696495</td>\n",
       "      <td>3621737643587579081</td>\n",
       "      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2...</td>\n",
       "      <td>NY</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp eventType            contentId             personId  \\\n",
       "0  1465413032      VIEW -3499919498720038879 -8845298781299428018   \n",
       "1  1465412560      VIEW  8890720798209849691 -1032019229384696495   \n",
       "\n",
       "             sessionId                                          userAgent  \\\n",
       "0  1264196770339959068                                                NaN   \n",
       "1  3621737643587579081  Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2...   \n",
       "\n",
       "  userRegion userCountry  \n",
       "0        NaN         NaN  \n",
       "1         NY          US  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df = pd.read_csv('users_interactions.csv')\n",
    "interactions_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data munging\n",
    "As there are different interactions types, we associate them with a weight or strength, assuming that, for example, a comment in an article indicates a higher interest of the user on the item than a like, or than a simple view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type_strength = {\n",
    "   'VIEW': 1.0,\n",
    "   'LIKE': 2.0, \n",
    "   'BOOKMARK': 2.5, \n",
    "   'FOLLOW': 3.0,\n",
    "   'COMMENT CREATED': 4.0,  \n",
    "}\n",
    "#Here we are creating the column event strenght by event type giving the weights for event type by domain know.\n",
    "interactions_df['eventStrength'] = interactions_df['eventType'].apply(lambda x: event_type_strength[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender systems have a problem known as user cold-start, in which is hard do provide personalized recommendations for users with none or a very few number of consumed items, due to the lack of information to model their preferences.\n",
    "For this reason, we are keeping in the dataset only users with at leas 5 interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cold start problem\n",
    "\n",
    "The problem\n",
    "Recommendation engines that run on collaborative filtering recommend each item (products advertised on your site) based on user actions. The more user actions an item has, the easier it is to tell which user would be interested in it and what other items are similar to it. As time progresses, the system will be able to give more and more accurate recommendations.\n",
    "\n",
    "This, however, brings a major contradiction and difficulty to classified sites and their recommendation engines. Even though the newest ads are actually the most relevant ones, a recommendation system has far less confidence in recommending them to your users than it has with older items, but it’s just simply not a good idea to let older ads dominate the recommendation process.\n",
    "\n",
    "The solution\n",
    "Content-based filtering is the method that answers this question. Our system first uses the metadata of new products when creating recommendations, while visitor action is secondary for a certain period of time.\n",
    "\n",
    "Also, we can identify visitors who are only there to browse and those determined visitors who know what they are looking for. For example, if someone clicks on everything from phone cases to real estate within a short period of time, the system will assume she is only there for browsing and won’t use their click history for recommendations.\n",
    "\n",
    "When it comes to investigating the cold start phenomenon, this is only the tip of the iceberg. Every recommendation solution has a different method to cope with it, and after getting over the rough cold start, the real work of the engine begins.\n",
    "\n",
    "Visitor cold start\n",
    "The user or visitor cold start simply means that a recommendation engine meets a new visitor for the first time. because there is no user history about her, the system doesn’t know the personal preferences of the user. Getting to know your visitors is crucial in creating a great user experience for them.\n",
    "\n",
    "Identifying visitors\n",
    "Visitors to your site will probably not register upon their very first arrival. It’s actually rather understandable, given the fact that most internet users have already signed up for numerous sites and don’t fancy the idea of going through yet another registration process just to have a look around. What’s more, classified ad sites usually don’t require visitors to be logged in. Your visitors don’t even need to have an account to contact the advertiser and then buy the advertised good. So, your users need to be identified by cookies.\n",
    "\n",
    "If visitors delete or block cookies, they will be identified as a brand new user each time they visit their favorite classified ads site. The classified site should try to give these users specific identifiers, like unique user IDs. If this cannot be solved, everything the recommendation engine learns about these users will be lost as soon as they leave the site.\n",
    "\n",
    "However, product-to-product recommendations will persist in any case, whether your users block cookies or not.\n",
    "\n",
    "A never-ending story\n",
    "While it’s true that cold start mostly affects new visitors, we must not forget that a similar phenomenon can easily happen with returning users. We shall illustrate what we mean with the help of the following example.\n",
    "\n",
    "Robert is looking for desks on your site. He’ll be interested in ads for desks for around a week, but after he finds the right one, he will move on. When he visits again in a month, he may be looking for lawn mowers.\n",
    "\n",
    "So the cold start problem exists all the time, as Robert (and any of your users) will always be interested in new and different things. At the start of each visit, the recommendation system does not know whether the user arrived with new ideas or if she is still looking for the earlier items. This is why it is important for the recommendation system to identify the user’s actual, active interest after the first few clicks. Also, note that certain topics of interest are more persistent than others. For example, collectibles (stamps, memorabilia, coins, books, etc.) exhibit a rather long interest span, compared to typical consumer goods like desks or lawn mowers.\n",
    "\n",
    "Multiple devices\n",
    "Even if users allow cookies in their browser, they can still experience the cold start. This can happen, for example, when they use someone else’s computer to browse, or simply if they have multiple devices. Classified sites cannot link their user history from their different devices if they don’t have an account.\n",
    "\n",
    "The solution\n",
    "Recommendation systems have an efficient solution for the visitor cold start problem. Below are the most important types of information that help minimize or eliminate the cold start phase. With the exception of behavioral information, all of this data can be obtained from both new visitors and returning users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender systems have a problem known as user cold-start, in which is hard do provide personalized recommendations for users with none or a very few number of consumed items, due to the lack of information to model their preferences.\n",
    "For this reason, we are keeping in the dataset only users with at leas 5 interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personId\n",
      "-9223121837663643404    43\n",
      "-9212075797126931087     5\n",
      "-9207251133131336884     7\n",
      "-9199575329909162940    11\n",
      "-9196668942822132778     7\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Here we are addressing cold start problem.\n",
    "users_interactions_count_df = interactions_df.groupby(['personId', 'contentId']).size().groupby('personId').size()\n",
    "#print('# users: %d' % len(users_interactions_count_df))\n",
    "print(users_interactions_count_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# users with at least 5 interactions: 1140\n"
     ]
    }
   ],
   "source": [
    "users_with_enough_interactions_df = users_interactions_count_df[users_interactions_count_df >= 5].reset_index()[['personId']]\n",
    "print('# users with at least 5 interactions: %d' % len(users_with_enough_interactions_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of interactions: 72312\n",
      "# of interactions from users with at least 5 interactions: 69868\n"
     ]
    }
   ],
   "source": [
    "print('# of interactions: %d' % len(interactions_df))\n",
    "interactions_from_selected_users_df = interactions_df.merge(users_with_enough_interactions_df, \n",
    "               how = 'right',\n",
    "               left_on = 'personId',\n",
    "               right_on = 'personId')\n",
    "print('# of interactions from users with at least 5 interactions: %d' % len(interactions_from_selected_users_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Deskdrop, users are allowed to view an article many times, and interact with them in different ways (eg. like or comment). Thus, to model the user interest on a given article, we aggregate all the interactions the user has performed in an item by a weighted sum of interaction type strength and apply a log transformation to smooth the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique user/item interactions: 39106\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>contentId</th>\n",
       "      <th>eventStrength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9223121837663643404</td>\n",
       "      <td>-8949113594875411859</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9223121837663643404</td>\n",
       "      <td>-8377626164558006982</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9223121837663643404</td>\n",
       "      <td>-8208801367848627943</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              personId            contentId  eventStrength\n",
       "0 -9223121837663643404 -8949113594875411859            1.0\n",
       "1 -9223121837663643404 -8377626164558006982            1.0\n",
       "2 -9223121837663643404 -8208801367848627943            1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def smooth_user_preference(x):\n",
    "    return math.log(1+x, 2)\n",
    "    \n",
    "interactions_full_df = interactions_from_selected_users_df \\\n",
    "                    .groupby(['personId', 'contentId'])['eventStrength'].sum() \\\n",
    "                    .apply(smooth_user_preference).reset_index()\n",
    "print('# of unique user/item interactions: %d' % len(interactions_full_df))\n",
    "interactions_full_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Evaluation is important for machine learning projects, because it allows to compare objectivelly different algorithms and hyperparameter choices for models.\n",
    "One key aspect of evaluation is to ensure that the trained model generalizes for data it was not trained on, using Cross-validation techniques. We are using here a simple cross-validation approach named holdout, in which a random data sample (20% in this case) are kept aside in the training process, and exclusively used for evaluation. All evaluation metrics reported here are computed using the test set.\n",
    "\n",
    "Ps. A more robust evaluation approach could be to split train and test sets by a reference date, where the train set is composed by all interactions before that date, and the test set are interactions after that date. For the sake of simplicity, we chose the first random approach for this notebook, but you may want to try the second approach to better simulate how the recsys would perform in production predicting \"future\" users interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# interactions on Train set: 31284\n",
      "# interactions on Test set: 7822\n"
     ]
    }
   ],
   "source": [
    "interactions_train_df, interactions_test_df = train_test_split(interactions_full_df,\n",
    "                                   stratify=interactions_full_df['personId'], \n",
    "                                   test_size=0.20,\n",
    "                                   random_state=42)\n",
    "\n",
    "print('# interactions on Train set: %d' % len(interactions_train_df))\n",
    "print('# interactions on Test set: %d' % len(interactions_test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Recommender Systems, there are a set metrics commonly used for evaluation. We chose to work with Top-N accuracy metrics, which evaluates the accuracy of the top recommendations provided to a user, comparing to the items the user has actually interacted in test set.\n",
    "This evaluation method works as follows:\n",
    "\n",
    "For each user\n",
    "For each item the user has interacted in test set\n",
    "Sample 100 other items the user has never interacted.\n",
    "Ps. Here we naively assume those non interacted items are not relevant to the user, which might not be true, as the user may simply not be aware of those not interacted items. But let's keep this assumption.\n",
    "Ask the recommender model to produce a ranked list of recommended items, from a set composed one interacted item and the 100 non-interacted (\"non-relevant!) items\n",
    "Compute the Top-N accuracy metrics for this user and interacted item from the recommendations ranked list\n",
    "Aggregate the global Top-N accuracy metrics\n",
    "The Top-N accuracy metric choosen was Recall@N which evaluates whether the interacted item is among the top N items (hit) in the ranked list of 101 recommendations for a user.\n",
    "Ps. Other popular ranking metrics are NDCG@N and MAP@N, whose score calculation takes into account the position of the relevant item in the ranked list (max. value if relevant item is in the first position). You can find a reference to implement this metrics in this post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                contentId  eventStrength\n",
      "personId                                                \n",
      "-9223121837663643404 -8949113594875411859            1.0\n",
      "-9223121837663643404 -8377626164558006982            1.0\n",
      "-9223121837663643404 -8208801367848627943            1.0\n"
     ]
    }
   ],
   "source": [
    "interactions_full_indexed_df = interactions_full_df.set_index('personId')\n",
    "interactions_train_indexed_df = interactions_train_df.set_index('personId')\n",
    "interactions_test_indexed_df = interactions_test_df.set_index('personId')\n",
    "\n",
    "print(interactions_full_indexed_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items_interacted(person_id, interactions_df):\n",
    "    interacted_items = interactions_df.loc[person_id]['contentId']\n",
    "    return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This chunk of code performs model validation.\n",
    "EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS = 100\n",
    "\n",
    "class ModelEvaluator:\n",
    "\n",
    "\n",
    "    def get_not_interacted_items_sample(self, person_id, sample_size, seed=42):\n",
    "        interacted_items = get_items_interacted(person_id, interactions_full_indexed_df)\n",
    "        all_items = set(articles_df['contentId'])\n",
    "        non_interacted_items = all_items - interacted_items\n",
    "\n",
    "        random.seed(seed)\n",
    "        non_interacted_items_sample = random.sample(non_interacted_items, sample_size)\n",
    "        return set(non_interacted_items_sample)\n",
    "\n",
    "    def _verify_hit_top_n(self, item_id, recommended_items, topn):        \n",
    "            try:\n",
    "                index = next(i for i, c in enumerate(recommended_items) if c == item_id)\n",
    "            except:\n",
    "                index = -1\n",
    "            hit = int(index in range(0, topn))\n",
    "            return hit, index\n",
    "\n",
    "    def evaluate_model_for_user(self, model, person_id):\n",
    "        #Getting the items in test set\n",
    "        interacted_values_testset = interactions_test_indexed_df.loc[person_id]\n",
    "        if type(interacted_values_testset['contentId']) == pd.Series:\n",
    "            person_interacted_items_testset = set(interacted_values_testset['contentId'])\n",
    "        else:\n",
    "            person_interacted_items_testset = set([int(interacted_values_testset['contentId'])])  \n",
    "        interacted_items_count_testset = len(person_interacted_items_testset) \n",
    "\n",
    "        #Getting a ranked recommendation list from a model for a given user\n",
    "        person_recs_df = model.recommend_items(person_id, \n",
    "                                               items_to_ignore=get_items_interacted(person_id, \n",
    "                                                                                    interactions_train_indexed_df), \n",
    "                                               topn=10000000000)\n",
    "\n",
    "        hits_at_5_count = 0\n",
    "        hits_at_10_count = 0\n",
    "        #For each item the user has interacted in test set\n",
    "        for item_id in person_interacted_items_testset:\n",
    "            #Getting a random sample (100) items the user has not interacted \n",
    "            #(to represent items that are assumed to be no relevant to the user)\n",
    "            non_interacted_items_sample = self.get_not_interacted_items_sample(person_id, \n",
    "                                                                          sample_size=EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS, \n",
    "                                                                          seed=item_id%(2**32))\n",
    "\n",
    "            #Combining the current interacted item with the 100 random items\n",
    "            items_to_filter_recs = non_interacted_items_sample.union(set([item_id]))\n",
    "\n",
    "            #Filtering only recommendations that are either the interacted item or from a random sample of 100 non-interacted items\n",
    "            valid_recs_df = person_recs_df[person_recs_df['contentId'].isin(items_to_filter_recs)]                    \n",
    "            valid_recs = valid_recs_df['contentId'].values\n",
    "            #Verifying if the current interacted item is among the Top-N recommended items\n",
    "            hit_at_5, index_at_5 = self._verify_hit_top_n(item_id, valid_recs, 5)\n",
    "            hits_at_5_count += hit_at_5\n",
    "            hit_at_10, index_at_10 = self._verify_hit_top_n(item_id, valid_recs, 10)\n",
    "            hits_at_10_count += hit_at_10\n",
    "\n",
    "        #Recall is the rate of the interacted items that are ranked among the Top-N recommended items, \n",
    "        #when mixed with a set of non-relevant items\n",
    "        recall_at_5 = hits_at_5_count / float(interacted_items_count_testset)\n",
    "        recall_at_10 = hits_at_10_count / float(interacted_items_count_testset)\n",
    "\n",
    "        person_metrics = {'hits@5_count':hits_at_5_count, \n",
    "                          'hits@10_count':hits_at_10_count, \n",
    "                          'interacted_count': interacted_items_count_testset,\n",
    "                          'recall@5': recall_at_5,\n",
    "                          'recall@10': recall_at_10}\n",
    "        return person_metrics\n",
    "\n",
    "    def evaluate_model(self, model):\n",
    "        #print('Running evaluation for users')\n",
    "        people_metrics = []\n",
    "        for idx, person_id in enumerate(list(interactions_test_indexed_df.index.unique().values)):\n",
    "            #if idx % 100 == 0 and idx > 0:\n",
    "            #    print('%d users processed' % idx)\n",
    "            person_metrics = self.evaluate_model_for_user(model, person_id)  \n",
    "            person_metrics['_person_id'] = person_id\n",
    "            people_metrics.append(person_metrics)\n",
    "        print('%d users processed' % idx)\n",
    "\n",
    "        detailed_results_df = pd.DataFrame(people_metrics) \\\n",
    "                            .sort_values('interacted_count', ascending=False)\n",
    "        \n",
    "        global_recall_at_5 = detailed_results_df['hits@5_count'].sum() / float(detailed_results_df['interacted_count'].sum())\n",
    "        global_recall_at_10 = detailed_results_df['hits@10_count'].sum() / float(detailed_results_df['interacted_count'].sum())\n",
    "        \n",
    "        global_metrics = {'modelName': model.get_model_name(),\n",
    "                          'recall@5': global_recall_at_5,\n",
    "                          'recall@10': global_recall_at_10}    \n",
    "        return global_metrics, detailed_results_df\n",
    "    \n",
    "model_evaluator = ModelEvaluator()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularity model\n",
    "A common (and usually hard-to-beat) baseline approach is the Popularity model. This model is not actually personalized - it simply recommends to a user the most popular items that the user has not previously consumed. As the popularity accounts for the \"wisdom of the crowds\", it usually provides good recommendations, generally interesting for most people.\n",
    "Ps. The main objective of a recommender system is to leverage the long-tail items to the users with very specific interests, which goes far beyond this simple technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contentId</th>\n",
       "      <th>eventStrength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4029704725707465084</td>\n",
       "      <td>307.733799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6783772548752091658</td>\n",
       "      <td>233.762157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-133139342397538859</td>\n",
       "      <td>228.024567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8208801367848627943</td>\n",
       "      <td>197.107608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6843047699859121724</td>\n",
       "      <td>193.825208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8224860111193157980</td>\n",
       "      <td>189.044680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-2358756719610361882</td>\n",
       "      <td>183.110951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2581138407738454418</td>\n",
       "      <td>180.282876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7507067965574797372</td>\n",
       "      <td>179.094002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1469580151036142903</td>\n",
       "      <td>170.548969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             contentId  eventStrength\n",
       "0 -4029704725707465084     307.733799\n",
       "1 -6783772548752091658     233.762157\n",
       "2  -133139342397538859     228.024567\n",
       "3 -8208801367848627943     197.107608\n",
       "4 -6843047699859121724     193.825208\n",
       "5  8224860111193157980     189.044680\n",
       "6 -2358756719610361882     183.110951\n",
       "7  2581138407738454418     180.282876\n",
       "8  7507067965574797372     179.094002\n",
       "9  1469580151036142903     170.548969"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the logic\n",
    "#Computes the most popular items\n",
    "item_popularity_df = interactions_full_df.groupby('contentId')['eventStrength'].sum().sort_values(ascending=False).reset_index()\n",
    "item_popularity_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityRecommender:\n",
    "    \n",
    "    MODEL_NAME = 'Popularity'\n",
    "    \n",
    "    def __init__(self, popularity_df, items_df=None):\n",
    "        self.popularity_df = popularity_df\n",
    "        self.items_df = items_df\n",
    "        \n",
    "    def get_model_name(self):\n",
    "        return self.MODEL_NAME\n",
    "        \n",
    "    def recommend_items(self, user_id, items_to_ignore=[], topn=10, verbose=False):\n",
    "        # Recommend the more popular items that the user hasn't seen yet.\n",
    "        recommendations_df = self.popularity_df[~self.popularity_df['contentId'].isin(items_to_ignore)] \\\n",
    "                               .sort_values('eventStrength', ascending = False) \\\n",
    "                               .head(topn)\n",
    "\n",
    "        if verbose:\n",
    "            if self.items_df is None:\n",
    "                raise Exception('\"items_df\" is required in verbose mode')\n",
    "\n",
    "            recommendations_df = recommendations_df.merge(self.items_df, how = 'left', \n",
    "                                                          left_on = 'contentId', \n",
    "                                                          right_on = 'contentId')[['eventStrength', 'contentId', 'title', 'url', 'lang']]\n",
    "\n",
    "\n",
    "        return recommendations_df\n",
    "    \n",
    "popularity_model = PopularityRecommender(item_popularity_df, articles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the Popularity model\n",
    "\n",
    "Here we perform the evaluation of the Popularity model, according to the method described above.\n",
    "It achieved the Recall@5 of 0.2417, which means that about 24% of interacted items in test set were ranked by Popularity model among the top-5 items (from lists with 100 random items). And Recall@10 was even higher (37%), as expected.\n",
    "It might be surprising to you that usually Popularity models could perform so well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Popularity recommendation model...\n",
      "1139 users processed\n",
      "\n",
      "Global metrics:\n",
      "{'modelName': 'Popularity', 'recall@5': 0.2418818716440808, 'recall@10': 0.3725389925850166}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_person_id</th>\n",
       "      <th>hits@10_count</th>\n",
       "      <th>hits@5_count</th>\n",
       "      <th>interacted_count</th>\n",
       "      <th>recall@10</th>\n",
       "      <th>recall@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>3609194402293569455</td>\n",
       "      <td>50</td>\n",
       "      <td>28</td>\n",
       "      <td>192</td>\n",
       "      <td>0.260417</td>\n",
       "      <td>0.145833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-2626634673110551643</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>134</td>\n",
       "      <td>0.186567</td>\n",
       "      <td>0.089552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1032019229384696495</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>130</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1443636648652872475</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>117</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.042735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-2979881261169775358</td>\n",
       "      <td>40</td>\n",
       "      <td>26</td>\n",
       "      <td>88</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.295455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>-3596626804281480007</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>80</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1116121227607581999</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>73</td>\n",
       "      <td>0.465753</td>\n",
       "      <td>0.273973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>692689608292948411</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>69</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.246377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-9016528795238256703</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>69</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.202899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3636910968448833585</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>68</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.308824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              _person_id  hits@10_count  hits@5_count  interacted_count  \\\n",
       "76   3609194402293569455             50            28               192   \n",
       "17  -2626634673110551643             25            12               134   \n",
       "16  -1032019229384696495             23            13               130   \n",
       "10  -1443636648652872475              9             5               117   \n",
       "82  -2979881261169775358             40            26                88   \n",
       "161 -3596626804281480007             18            12                80   \n",
       "65   1116121227607581999             34            20                73   \n",
       "81    692689608292948411             23            17                69   \n",
       "106 -9016528795238256703             18            14                69   \n",
       "52   3636910968448833585             28            21                68   \n",
       "\n",
       "     recall@10  recall@5  \n",
       "76    0.260417  0.145833  \n",
       "17    0.186567  0.089552  \n",
       "16    0.176923  0.100000  \n",
       "10    0.076923  0.042735  \n",
       "82    0.454545  0.295455  \n",
       "161   0.225000  0.150000  \n",
       "65    0.465753  0.273973  \n",
       "81    0.333333  0.246377  \n",
       "106   0.260870  0.202899  \n",
       "52    0.411765  0.308824  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Evaluating Popularity recommendation model...')\n",
    "pop_global_metrics, pop_detailed_results_df = model_evaluator.evaluate_model(popularity_model)\n",
    "print('\\nGlobal metrics:\\n%s' % pop_global_metrics)\n",
    "pop_detailed_results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-Based Filtering model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-based filtering approaches leverage description or attributes from items the user has interacted to recommend similar items. It depends only on the user previous choices, making this method robust to avoid the cold-start problem. For textual items, like articles, news and books, it is simple to use the raw text to build item profiles and user profiles.\n",
    "Here we are using a very popular technique in information retrieval (search engines) named TF-IDF. This technique converts unstructured text into a vector structure, where each word is represented by a position in the vector, and the value measures how relevant a given word is for an article. As all items will be represented in the same Vector Space Model, it is to compute similarity between articles.\n",
    "See this presentation (from slide 30) for more information on TF-IDF and Cosine similarity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
