#Data-Preprocessing: removing '\n'
FCA_html<-gsub("\n","",FCA_html)
#remove all round brackets
FCA_html<-FCA_html%>%str_replace_all("\\(|\\)", "")
#install.packages("pacman")
library(pacman)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
#install.packages("ggraph")
library(ggraph)
library(ggplot2)
library(plotly)
#install.packages("tm")
library(tm)
#install.packages("tidyquant")
#library(tidyquant)
#install.packages(tidytext)
library(tidytext)
#install.packages("RTextTools")
#install.packages("RTextTools", repos = "http://r.findata.org")
#install.packages("RTextTools", dependencies=TRUE, repos='http://cran.rstudio.com/')
#install.packages('RTextTools',repos='http://cran.us.r-project.org')
library(SnowballC)
library(topicmodels)
library(ggthemes)
#nstall.packages("waff")
library(waff)
install.packages("waff")
#install.packages("waff")
library(waff)
library(grid)
library(gridExtra)
#install.packages("textreadr")
library(textreadr)
#install.packages("xml2",'https://cran.rstudio.com/bin/windows/contrib/3.5/xml2_1.2.0.zip')
library(purrr)
library(xml2)
#install.packages("rvest")
library(rvest)
#Data-Preprocessing: removing '\n'
FCA_html<-gsub("\n","",FCA_html)
#remove all round brackets
FCA_html<-FCA_html%>%str_replace_all("\\(|\\)", "")
library(stringr)
#install.packages("pacman")
library(pacman)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
#install.packages("ggraph")
library(ggraph)
library(ggplot2)
library(plotly)
#install.packages("tm")
library(tm)
#install.packages("tidyquant")
#library(tidyquant)
#install.packages(tidytext)
library(tidytext)
#install.packages("RTextTools")
#install.packages("RTextTools", repos = "http://r.findata.org")
#install.packages("RTextTools", dependencies=TRUE, repos='http://cran.rstudio.com/')
#install.packages('RTextTools',repos='http://cran.us.r-project.org')
library(SnowballC)
library(topicmodels)
library(ggthemes)
#install.packages("waff")
library(waff)
#install.packages("pacman")
library(pacman)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
#install.packages("ggraph")
library(ggraph)
library(ggplot2)
library(plotly)
#install.packages("tm")
library(tm)
#install.packages("tidyquant")
#library(tidyquant)
#install.packages(tidytext)
library(tidytext)
#install.packages("RTextTools")
#install.packages("RTextTools", repos = "http://r.findata.org")
#install.packages("RTextTools", dependencies=TRUE, repos='http://cran.rstudio.com/')
#install.packages('RTextTools',repos='http://cran.us.r-project.org')
library(SnowballC)
library(topicmodels)
library(ggthemes)
#install.packages("waff")
#library(waff)
library(grid)
library(gridExtra)
#install.packages("textreadr")
library(textreadr)
#install.packages("xml2",'https://cran.rstudio.com/bin/windows/contrib/3.5/xml2_1.2.0.zip')
library(purrr)
library(xml2)
library(stringr)
#install.packages("rvest")
library(rvest)
#Data-Preprocessing: removing '\n'
FCA_html<-gsub("\n","",FCA_html)
#remove all round brackets
FCA_html<-FCA_html%>%str_replace_all("\\(|\\)", "")
#remove all \\
FCA_html<-FCA_html%>%str_replace_all("\\\\", "")
#remove all non words and non numbers
#FCA_html<-FCA_html%>%str_replace_all("[^A-Za-z0-9]", "")
#remove all •
FCA_html<-FCA_html%>%str_replace_all("\\•  ", "")
#remove all &
FCA_html<-FCA_html%>%str_replace_all("\\ & ", "")
#remove all  non printable words
FCA_html<-FCA_html%>%str_replace_all("[^[:print:]]", "")
#remove all \
FCA_html<-FCA_html%>%str_replace_all(pattern = "\"", replacement = "")
#FCAindeed2<-FCAindeed2%>%stringi::stri_unescape_unicode()
# remove digits
#FCA_html%>%str_replace_all(pattern = "[[:digit:]]+", replacement = "")
#tm::removeNumbers(FCA_html)
#### pattern for dates
pattern ="\\(?\\d{4}\\)?[.-]? *\\d{2}[.-]? *[.-]?\\d{2}"
date=FCA_html%>%str_extract_all(pattern)
#FCA_html[[1]]%>%str_subset(pattern = "([0-9]{1,2})[- .]([a-zA-Z]+)[- .]([0-9]{4})")
#FCA_html[[1]]
#unlist(Date)
Date=as.Date(unlist(date))
#FCA_html_2=data_frame(Date=as.Date(unlist(date)),FCA_html)
get_sentiments(lexicon = "nrc")%>%
count(sentiment, sort = TRUE)
library(plyr)
get_sentiments(lexicon = "nrc")%>%
count(sentiment, sort = TRUE)
#Data-Preprocessing: removing '\n'
FCA_html<-gsub("\n","",FCA_html)
#remove all round brackets
FCA_html<-FCA_html%>%str_replace_all("\\(|\\)", "")
#remove all \\
FCA_html<-FCA_html%>%str_replace_all("\\\\", "")
#remove all non words and non numbers
#FCA_html<-FCA_html%>%str_replace_all("[^A-Za-z0-9]", "")
#remove all •
FCA_html<-FCA_html%>%str_replace_all("\\•  ", "")
#remove all &
FCA_html<-FCA_html%>%str_replace_all("\\ & ", "")
#remove all  non printable words
FCA_html<-FCA_html%>%str_replace_all("[^[:print:]]", "")
#remove all \
FCA_html<-FCA_html%>%str_replace_all(pattern = "\"", replacement = "")
#FCAindeed2<-FCAindeed2%>%stringi::stri_unescape_unicode()
# remove digits
#FCA_html%>%str_replace_all(pattern = "[[:digit:]]+", replacement = "")
#tm::removeNumbers(FCA_html)
#### pattern for dates
pattern ="\\(?\\d{4}\\)?[.-]? *\\d{2}[.-]? *[.-]?\\d{2}"
date=FCA_html%>%str_extract_all(pattern)
#FCA_html[[1]]%>%str_subset(pattern = "([0-9]{1,2})[- .]([a-zA-Z]+)[- .]([0-9]{4})")
#FCA_html[[1]]
#unlist(Date)
#FCA_html_2=data_frame(Date=as.Date(unlist(date)),FCA_html)
FCA_html<-ht
#Data-Preprocessing: removing '\n'
FCA_html<-gsub("\n","",FCA_html)
#remove all round brackets
FCA_html<-FCA_html%>%str_replace_all("\\(|\\)", "")
#remove all \\
FCA_html<-FCA_html%>%str_replace_all("\\\\", "")
#remove all non words and non numbers
#FCA_html<-FCA_html%>%str_replace_all("[^A-Za-z0-9]", "")
#remove all •
FCA_html<-FCA_html%>%str_replace_all("\\•  ", "")
#remove all &
FCA_html<-FCA_html%>%str_replace_all("\\ & ", "")
#remove all  non printable words
FCA_html<-FCA_html%>%str_replace_all("[^[:print:]]", "")
#remove all \
FCA_html<-FCA_html%>%str_replace_all(pattern = "\"", replacement = "")
#FCAindeed2<-FCAindeed2%>%stringi::stri_unescape_unicode()
# remove digits
#FCA_html%>%str_replace_all(pattern = "[[:digit:]]+", replacement = "")
#tm::removeNumbers(FCA_html)
#### pattern for dates
pattern ="\\(?\\d{4}\\)?[.-]? *\\d{2}[.-]? *[.-]?\\d{2}"
date=FCA_html%>%str_extract_all(pattern)
#FCA_html[[1]]%>%str_subset(pattern = "([0-9]{1,2})[- .]([a-zA-Z]+)[- .]([0-9]{4})")
#FCA_html[[1]]
#unlist(Date)
#FCA_html_2=data_frame(Date=as.Date(unlist(date)),FCA_html)
get_sentiments(lexicon = "nrc")%>%
count(sentiment, sort = TRUE)
get_sentiments(lexicon = "nrc")%>%
count(sentiment, sort = TRUE)
get_sentiments(lexicon = "nrc")%>%
count(sentiment)
#install.packages("pacman")
library(pacman)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
#install.packages("ggraph")
library(ggraph)
library(ggplot2)
library(plotly)
#install.packages("tm")
library(tm)
#install.packages("tidyquant")
#library(tidyquant)
#install.packages(tidytext)
library(tidytext)
#install.packages("RTextTools")
#install.packages("RTextTools", repos = "http://r.findata.org")
#install.packages("RTextTools", dependencies=TRUE, repos='http://cran.rstudio.com/')
#install.packages('RTextTools',repos='http://cran.us.r-project.org')
library(SnowballC)
library(topicmodels)
library(ggthemes)
#install.packages("waff")
#library(waff)
library(grid)
library(gridExtra)
#install.packages("textreadr")
library(textreadr)
#install.packages("xml2",'https://cran.rstudio.com/bin/windows/contrib/3.5/xml2_1.2.0.zip')
library(purrr)
library(xml2)
library(stringr)
#install.packages("rvest")
library(rvest)
library(plyr)
get_sentiments(lexicon = "nrc")%>%
count(sentiment,)
get_sentiments(lexicon = "nrc")
GlassdoorPages <- data_frame(date=as.Date(unlist(date)),page = seq(1, n),
text = c(FCA_html))%>%arrange(desc(date))
GlassdoorPages <- data_frame(page = seq(1, n),
text = c(FCA_html)))
GlassdoorPages <- data_frame(page = seq(1, n),
text = c(FCA_html))
GlassdoorPages <- data_frame(text = c(FCA_html))
library(tibble)
GlassdoorPages <- data_frame(text = c(FCA_html))
GlassdoorPages <- tibble(text = c(FCA_html))
View(GlassdoorPages)
indeed <- tibble(text = c(FCA_html))
#convert all text to lower case
df_lower<- tolower(indeed$text)
# Replace blank space (“rt”)
df_lower <- gsub("rt", "", df_lower)
# Replace @UserName
df_lower <- gsub("@\\w+", "", df_lower)
# Remove punctuation
df_lower <- gsub("[[:punct:]]", "", df_lower)
# Remove links
df_lower <- gsub("http\\w+", "", df_lower)
# Remove tabs
df_lower<- gsub("[ |\t]{2,}", "", df_lower)
# Remove blank spaces at the beginning
df_lower <- gsub("^ ", "", df_lower)
# Remove blank spaces at the end
df_lower <- gsub(" $", "",df_lower )
#clean up by removing stop words
corpus<-Corpus(VectorSource(df_lower))
corpus <- tm_map(corpus, function(x)removeWords(x,stopwords()))
#generate wordcloud
wordcloud(corpus,min.freq = 10,colors=brewer.pal(8, "Dark2"),random.color = TRUE,max.words = 500)
library(wordcloud)
#generate wordcloud
wordcloud(corpus,min.freq = 10,colors=brewer.pal(8, "Dark2"),random.color = TRUE,max.words = 500)
#generate wordcloud
wordcloud(corpus,min.freq = 10,colors=brewer.pal(8, "Dark2"),random.color = TRUE,max.words = 500)
#getting emotions using in-built function
mysentiment<-get_nrc_sentiment((df_lower))
#install.packages("pacman")
library(pacman)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
#install.packages("ggraph")
library(ggraph)
library(ggplot2)
library(plotly)
#install.packages("tm")
library(tm)
#install.packages("tidyquant")
#library(tidyquant)
#install.packages(tidytext)
library(tidytext)
#install.packages("RTextTools")
#install.packages("RTextTools", repos = "http://r.findata.org")
#install.packages("RTextTools", dependencies=TRUE, repos='http://cran.rstudio.com/')
#install.packages('RTextTools',repos='http://cran.us.r-project.org')
library(SnowballC)
library(topicmodels)
library(ggthemes)
#install.packages("waff")
#library(waff)
library(grid)
library(gridExtra)
#install.packages("textreadr")
library(textreadr)
#install.packages("xml2",'https://cran.rstudio.com/bin/windows/contrib/3.5/xml2_1.2.0.zip')
library(purrr)
library(xml2)
library(stringr)
#install.packages("rvest")
library(rvest)
library(plyr)
library(tibble)
library(wordcloud)
library(syuzhet)
#getting emotions using in-built function
mysentiment<-get_nrc_sentiment((df_lower))
#calculationg total score for each sentiment
Sentimentscores<-data.frame(colSums(mysentiment[,]))
names(Sentimentscores)<-"Score"
Sentimentscores<-cbind("sentiment"=rownames(Sentimentscores),Sentimentscores)
rownames(Sentimentscores)<-NULL
#plotting the sentiments with scores
ggplotly(ggplot(data=Sentimentscores,aes(x=reorder(sentiment,Score),y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people behind the tweets on #MachineLearning")+labs(title = "Sentiments of people behind the tweets on #MachineLearning",x="Sentiments", y = "Scores")+theme(plot.title = element_text(hjust = -10,face = "bold",color = "black"))+coord_flip())
#plotting the sentiments with scores
ggplotly(ggplot(data=Sentimentscores,aes(x=reorder(sentiment,Score),y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of Employees behind the comments or rating ")+labs(title = "Sentiments of people behind the tweets on #MachineLearning",x="Sentiments", y = "Scores")+theme(plot.title = element_text(hjust = -10,face = "bold",color = "black"))+coord_flip())
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d<-as.data.table(d)
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d<- d[with(d,order(-freq)),]
head(d, 20)
ggplotly(ggplot(d[1:10,],aes(x=reorder(word,freq),y=freq,fill=as.factor(word)))+geom_bar(stat = "identity")+ geom_text(aes(label=freq), vjust=10)+labs(title = "The Top 10 frequently used words",x="Words", y = "Frequency of Words",fill="Words")+theme(plot.title = element_text(hjust = -10,face = "bold",color = "black"))+coord_flip())
Top 10 words in terms of percentage.
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
d[1:10,]
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6)
p
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
library(dplyr)
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = word, values = d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
#Donet chart which display the percentage of each class in the column
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
#Donet chart which display the percentage of each class in the column
zz<- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
zz
#install.packages("pacman")
library(pacman)
library(dplyr)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
#install.packages("ggraph")
library(ggraph)
library(ggplot2)
library(plotly)
#install.packages("tm")
library(tm)
#install.packages("tidyquant")
library(tidyquant)
#install.packages(tidytext)
library(tidytext)
#install.packages("RTextTools")
#install.packages("RTextTools", repos = "http://r.findata.org")
#install.packages("RTextTools", dependencies=TRUE, repos='http://cran.rstudio.com/')
#install.packages('RTextTools',repos='http://cran.us.r-project.org')
library(SnowballC)
library(topicmodels)
#library(ggthemes)
#nstall.packages("waff")
#library(waff)
library(grid)
library(gridExtra)
library(dplyr)
pacman::p_load(tidyverse,tidytext,viridis,rvest,tm,wordcloud,SnowballC,tidyquant,ggridges,scales,highcharter,topicmodels)
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
library(data.table)
library(data.table)
d<-as.data.table(d)
p <- d[1:10,] %>%
group_by(word) %>%
plot_ly(labels = ~word, values = ~d[1:10,freq]) %>%
add_pie(hole = 0.6) %>%
layout(title = "The Percentage of top 10 frequently occuring words.",  showlegend = F,
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
n=507
#The reviews has 507 pages,thus n=507
urls <- paste0("https://www.glassdoor.co.in/Reviews/Northwestern-Mutual-Reviews-E2919_P",seq(2, n), ".htm?filter.defaultEmploymentStatuses=false&filter.defaultLocation=false")
urls<-c("https://www.glassdoor.co.in/Reviews/Northwestern-Mutual-Reviews-E2919.htm?filter.defaultEmploymentStatuses=false&filter.defaultLocation=false",urls)
html <- urls %>%
map_chr(~ read_html(.) %>% html_node(".hreview")%>%html_text())
