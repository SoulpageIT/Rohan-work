library(highcharter)
#install.packages("ggraph")
library(ggraph)
library(ggplot2)
library(plotly)
#install.packages("tm")
#library(tm)
#install.packages("tidyquant")
#library(tidyquant)
#install.packages(tidytext)
library(tidytext)
#install.packages("RTextTools")
#install.packages("RTextTools", repos = "http://r.findata.org")
#install.packages("RTextTools", dependencies=TRUE, repos='http://cran.rstudio.com/')
#install.packages('RTextTools',repos='http://cran.us.r-project.org')
library(SnowballC)
#library(topicmodels)
#library(ggthemes)
#nstall.packages("waff")
#library(waff)
library(grid)
library(gridExtra)
#install.packages("textreadr")
library(textreadr)
#install.packages("xml2",'https://cran.rstudio.com/bin/windows/contrib/3.5/xml2_1.2.0.zip')
library(purrr)
n=135
#The reviews has 507 pages,thus n=507
urls <- paste0("https://www.indeed.co.in/cmp/Northwestern-Mutual/reviews?fcountry=ALL",seq(2, n), ".htm")
urls<-c("https://www.indeed.co.in/cmp/Northwestern-Mutual/reviews?fcountry=ALL",urls)
html <- urls %>%
map_chr(~ read_html(.) %>% html_node(".div span")%>%html_text())
#install.packages("pacman")
library(pacman)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
#install.packages("ggraph")
library(ggraph)
library(ggplot2)
library(plotly)
#install.packages("tm")
library(tm)
#install.packages("tidyquant")
library(tidyquant)
#install.packages(tidytext)
library(tidytext)
#install.packages("RTextTools")
#install.packages("RTextTools", repos = "http://r.findata.org")
#install.packages("RTextTools", dependencies=TRUE, repos='http://cran.rstudio.com/')
#install.packages('RTextTools',repos='http://cran.us.r-project.org')
library(SnowballC)
library(topicmodels)
#library(ggthemes)
#nstall.packages("waff")
#library(waff)
library(grid)
library(gridExtra)
n=507
#The reviews has 507 pages,thus n=507
urls <- paste0("https://www.glassdoor.co.in/Reviews/Northwestern-Mutual-Reviews-E2919_P",seq(2, n), ".htm?filter.defaultEmploymentStatuses=false&filter.defaultLocation=false")
urls<-c("https://www.glassdoor.co.in/Reviews/Northwestern-Mutual-Reviews-E2919.htm?filter.defaultEmploymentStatuses=false&filter.defaultLocation=false",urls)
html <- urls %>%
map_chr(~ read_html(.) %>% html_node(".hreview")%>%html_text())
#install.packages("pacman")
library(pacman)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
#install.packages("ggraph")
library(ggraph)
library(ggplot2)
library(plotly)
#install.packages("tm")
library(tm)
#install.packages("tidyquant")
library(tidyquant)
#install.packages(tidytext)
library(tidytext)
#install.packages("RTextTools")
#install.packages("RTextTools", repos = "http://r.findata.org")
#install.packages("RTextTools", dependencies=TRUE, repos='http://cran.rstudio.com/')
#install.packages('RTextTools',repos='http://cran.us.r-project.org')
library(SnowballC)
library(topicmodels)
#library(ggthemes)
#nstall.packages("waff")
#library(waff)
library(grid)
library(gridExtra)
pacman::p_load(tidyverse,tidytext,viridis,rvest,tm,wordcloud,SnowballC,tidyquant,ggridges,scales,highcharter,topicmodels)
install.packages("pacman")
pacman::p_load(tidyverse,tidytext,viridis,rvest,tm,wordcloud,SnowballC,tidyquant,ggridges,scales,highcharter,topicmodels)
n=507
#The reviews has 507 pages,thus n=507
urls <- paste0("https://www.glassdoor.co.in/Reviews/Northwestern-Mutual-Reviews-E2919_P",seq(2, n), ".htm?filter.defaultEmploymentStatuses=false&filter.defaultLocation=false")
urls<-c("https://www.glassdoor.co.in/Reviews/Northwestern-Mutual-Reviews-E2919.htm?filter.defaultEmploymentStatuses=false&filter.defaultLocation=false",urls)
html <- urls %>%
map_chr(~ read_html(.) %>% html_node(".hreview")%>%html_text())
library(dplyr)
library(dplyr)
install.packages("dplyr")
library(dplyr)
install.packages("dplyr")
library(dplyr)
#install.packages("pacman")
library(pacman)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
#install.packages("pacman")
library(pacman)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
install.packages("raster")
#install.packages("pacman")
library(pacman)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
library(dplyr)
install.packages("dplyr")
#install.packages("pacman")
library(pacman)
library(dplyr)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
#install.packages("ggraph")
library(ggraph)
library(ggplot2)
library(plotly)
#install.packages("tm")
library(tm)
#install.packages("pacman")
library(pacman)
library(dplyr)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
#install.packages("ggraph")
library(ggraph)
library(ggplot2)
library(plotly)
#install.packages("tm")
library(tm)
library(igraph)
library(ggraph)
library(plotly)
library(grid)
library(gridExtra)
library(tidyquant)
library(igraph)
library(ggraph)
library(plotly)
library(grid)
library(gridExtra)
library(tidyquant)
library(igraph)
library(ggraph)
library(plotly)
library(grid)
library(gridExtra)
library(tidyquant)
library(igraph)
library(ggraph)
library(plotly)
library(grid)
library(gridExtra)
#library(tidyquant)
library(tm)
pacman::p_load(tidyverse,tidytext,viridis,rvest,tm,wordcloud,SnowballC,tidyquant,ggridges,scales,highcharter,topicmodels)
library(igraph)
library(ggraph)
library(plotly)
library(grid)
library(gridExtra)
library(tidyquant)
library(tidyquant)
install.packages(c("progress", "raster", "reprex"))
install.packages("raster")
install.packages("raster")
library(igraph)
library(ggraph)
library(plotly)
library(grid)
library(gridExtra)
library(tidyquant)
install.packages("tidyquant")
library(igraph)
library(ggraph)
library(plotly)
library(grid)
library(gridExtra)
library(tidyquant)
library(igraph)
library(ggraph)
library(plotly)
library(grid)
library(gridExtra)
library(tidyquant)
library(tm)
pacman::p_load(tidyverse,tidytext,viridis,rvest,tm,wordcloud,SnowballC,tidyquant,ggridges,scales,highcharter,topicmodels)
load("NW mutual review.RData")
#Data-Preprocessing: removing '\n'
html<-gsub("\n","",html)
#remove all round brackets
html<-html%>%str_replace_all("\\(|\\)", "")
#remove all \\
html<-html%>%str_replace_all("\\\\", "")
#remove all non words and non numbers
#html<-html%>%str_replace_all("[^A-Za-z0-9]", "")
#remove all •
html<-html%>%str_replace_all("\\•  ", "")
#remove all &
html<-html%>%str_replace_all("\\ & ", "")
#remove all  non printable words
html<-html%>%str_replace_all("[^[:print:]]", "")
#remove all \
html<-html%>%str_replace_all(pattern = "\"", replacement = "")
#FCAindeed2<-FCAindeed2%>%stringi::stri_unescape_unicode()
# remove digits
#html%>%str_replace_all(pattern = "[[:digit:]]+", replacement = "")
#tm::removeNumbers(html)
#### pattern for dates
pattern ="\\(?\\d{4}\\)?[.-]? *\\d{2}[.-]? *[.-]?\\d{2}"
date=html%>%str_extract_all(pattern)
#html[[1]]%>%str_subset(pattern = "([0-9]{1,2})[- .]([a-zA-Z]+)[- .]([0-9]{4})")
#html[[1]]
#unlist(Date)
Date=as.Date(unlist(date))
#html_2=data_frame(Date=as.Date(unlist(date)),html)
GlassdoorPages <- tibble(date=as.Date(unlist(date)),page = seq(1, n),
text = c(html))%>%arrange(desc(date))
tidy <- GlassdoorPages %>%
unnest_tokens(word, text) %>%
add_count(date) %>%
dplyr::rename(date_total = n)
#remove stop words
data("stop_words")
tidy <- tidy %>%
anti_join(stop_words)
stop_user=c("linklink","whatsappshar","auburn","twittershar","edit","delet","via","edit","delet","via","starstarstarstarstarwork","pdt","hill","ago",
"facebookshar")
stop_user2=tibble(word=stop_user)
tidy <- tidy %>%
anti_join(stop_user2)
sentiment <- tidy %>%
inner_join(get_sentiments("bing"))
t<-list(
family="sans serif",
size=14,
color='black'
)
p <- sentiment %>%
group_by(sentiment) %>%
summarize(count = n()) %>%
plot_ly(labels = ~sentiment, values = ~count) %>%
add_pie(hole = 0.6) %>%
layout(title ,  showlegend = T,font=t,
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
sentiment <- tidy %>%
inner_join(get_sentiments("bing"))
theme_set(theme_bw())
#Alternatively
qq<-ggplotly(sentiment %>%
count(date, date_total, sentiment) %>%
filter(sentiment %in% c("positive", "negative"))%>%
mutate(sentiment = as.factor(sentiment)) %>%
#ggplot(aes(page, n / page_total, fill = sentiment)) +
ggplot(aes(date, n / sum(n),fill=sentiment))+
geom_area(position = "identity", alpha = 0.5) +
labs(x = "Year",y="Relative Frequency",
title = "Yearly Analysis of Sentiments of Employees",
subtitle = "Using the nrc lexicon")+theme_bw()+
scale_fill_manual(values=viridis_pal(option = "D")(6))+theme(plot.title = element_text(hjust = -10,face = "bold",color = "black",size = 14))+
scale_y_continuous(labels = scales::percent))
qq
p<-GlassdoorPages %>%
unnest_tokens(word, text)%>%
inner_join(get_sentiments("bing"))
library(igraph)
library(ggraph)
library(plotly)
library(grid)
library(gridExtra)
library(tidyquant)
library(tm)
pacman::p_load(tidyverse,tidytext,viridis,rvest,tm,wordcloud,SnowballC,tidyquant,ggridges,scales,highcharter,topicmodels)
load("NW mutual review.RData")
#Data-Preprocessing: removing '\n'
html<-gsub("\n","",html)
#remove all round brackets
html<-html%>%str_replace_all("\\(|\\)", "")
#remove all \\
html<-html%>%str_replace_all("\\\\", "")
#remove all non words and non numbers
#html<-html%>%str_replace_all("[^A-Za-z0-9]", "")
#remove all •
html<-html%>%str_replace_all("\\•  ", "")
#remove all &
html<-html%>%str_replace_all("\\ & ", "")
#remove all  non printable words
html<-html%>%str_replace_all("[^[:print:]]", "")
#remove all \
html<-html%>%str_replace_all(pattern = "\"", replacement = "")
#FCAindeed2<-FCAindeed2%>%stringi::stri_unescape_unicode()
# remove digits
#html%>%str_replace_all(pattern = "[[:digit:]]+", replacement = "")
#tm::removeNumbers(html)
#### pattern for dates
pattern ="\\(?\\d{4}\\)?[.-]? *\\d{2}[.-]? *[.-]?\\d{2}"
date=html%>%str_extract_all(pattern)
#html[[1]]%>%str_subset(pattern = "([0-9]{1,2})[- .]([a-zA-Z]+)[- .]([0-9]{4})")
#html[[1]]
#unlist(Date)
Date=as.Date(unlist(date))
#html_2=data_frame(Date=as.Date(unlist(date)),html)
GlassdoorPages <- tibble(date=as.Date(unlist(date)),page = seq(1, n),
text = c(html))%>%arrange(desc(date))
tidy <- GlassdoorPages %>%
unnest_tokens(word, text) %>%
add_count(date) %>%
dplyr::rename(date_total = n)
#remove stop words
data("stop_words")
tidy <- tidy %>%
anti_join(stop_words)
stop_user=c("linklink","whatsappshar","auburn","twittershar","edit","delet","via","edit","delet","via","starstarstarstarstarwork","pdt","hill","ago",
"facebookshar")
stop_user2=tibble(word=stop_user)
tidy <- tidy %>%
anti_join(stop_user2)
sentiment <- tidy %>%
inner_join(get_sentiments("bing"))
t<-list(
family="sans serif",
size=14,
color='black'
)
p <- sentiment %>%
group_by(sentiment) %>%
summarize(count = n()) %>%
plot_ly(labels = ~sentiment, values = ~count) %>%
add_pie(hole = 0.6) %>%
layout(title ,  showlegend = T,font=t,
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
sentiment <- tidy %>%
inner_join(get_sentiments("bing"))
t<-list(
family="sans serif",
size=14,
color='black'
)
p <- sentiment %>%
group_by(sentiment) %>%
summarize(count = n()) %>%
plot_ly(labels = ~sentiment, values = ~count) %>%
add_pie(hole = 0.6) %>%
layout(title ,  showlegend = T,font=t,
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
sentiment <- tidy %>%
inner_join(get_sentiments("bing"))
theme_set(theme_bw())
#Alternatively
qq<-ggplotly(sentiment %>%
count(date, date_total, sentiment) %>%
filter(sentiment %in% c("positive", "negative"))%>%
mutate(sentiment = as.factor(sentiment)) %>%
#ggplot(aes(page, n / page_total, fill = sentiment)) +
ggplot(aes(date, n / sum(n),fill=sentiment))+
geom_area(position = "identity", alpha = 0.5) +
labs(x = "Year",y="Relative Frequency",
title = "Yearly Analysis of Sentiments of Employees",
subtitle = "Using the nrc lexicon")+theme_bw()+
scale_fill_manual(values=viridis_pal(option = "D")(6))+theme(plot.title = element_text(hjust = -10,face = "bold",color = "black",size = 14))+
scale_y_continuous(labels = scales::percent))
qq
p<-GlassdoorPages %>%
unnest_tokens(word, text)%>%
inner_join(get_sentiments("bing"))
p%>%
count(word, sentiment, sort = TRUE) %>%
reshape2::acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = viridis_pal(option = "D")(2),
max.words = 100)
q<-sentiment %>%
count(sentiment, word) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
mutate(word = reorder(word, n)) %>%
mutate(sentiment = as.factor(sentiment))
q%>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(alpha = 0.8, show.legend = FALSE) +
coord_flip() +
scale_y_continuous(expand = c(0,0)) +
facet_wrap(~sentiment,scales="free") +
labs(y = "Total number of occurrences", x = "")+
#scale_fill_manual(values=viridis_pal(option = "D")(8))+
theme(plot.title = element_text(hjust = -10,face = "bold",color = "black",size = 14))+
scale_fill_viridis(end = 0.75, discrete=TRUE, direction = -1) +
scale_x_discrete(expand=c(0.02,0))  +
# strip horizontal  axis labels
theme(axis.title.x=element_blank()) +
theme(axis.ticks.x=element_blank()) +
theme(axis.text.x=element_blank())+
theme_minimal(base_size = 13)
sentiment <- tidy %>%
inner_join(get_sentiments("nrc"))
p <- sentiment %>%
group_by(sentiment) %>%
summarize(count = n()) %>%
plot_ly(labels = ~sentiment, values = ~count) %>%
add_pie(hole = 0.6) %>%
layout(showlegend = T,
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
p
sentiment <- tidy %>%
inner_join(get_sentiments("nrc"))
theme_set(theme_bw())
#Alternatively
ggplotly(sentiment %>%
count(date, date_total, sentiment) %>%
filter(sentiment %in% c("positive", "negative",
"joy", "trust","fear","sadness"))%>%
mutate(sentiment = as.factor(sentiment)) %>%
#ggplot(aes(page, n / page_total, fill = sentiment)) +
ggplot(aes(date, n / sum(n),fill=sentiment))+
geom_area(position = "identity", alpha = 0.5) +
labs(x = "Year",y="Relative Frequency",
title = "Yearly Analysis of Sentiments of Employees",
subtitle = "Using the nrc lexicon")+theme_bw()+
scale_fill_manual(values=viridis_pal(option = "D")(6))+theme(plot.title = element_text(hjust = -10,face = "bold",color = "black",size = 14))+
scale_y_continuous(labels = scales::percent))
p<-GlassdoorPages %>%
unnest_tokens(word, text)%>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment, sort = TRUE)
p%>%
filter(sentiment %in% c("negative","positive","joy","sadness"))%>%
reshape2::acast(word ~ sentiment, value.var = "n", fill = 0)%>%
comparison.cloud(colors = viridis_pal(option = "D")(4),
max.words = 200)
p<-sentiment %>%
count(sentiment, word) %>%
filter(sentiment %in% c("positive", "negative",
"joy", "trust","fear","sadness")) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup %>%
mutate(word = reorder(word, n)) %>%
mutate(sentiment = as.factor(sentiment))
p %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_bar(alpha = 0.8, show.legend = FALSE,stat = "identity") +
coord_flip() +
scale_y_continuous(expand = c(0,0)) +
facet_wrap(~sentiment, scales = "free") +
labs(y = "Total number of occurrences", x = "Words")+theme_bw()+
scale_fill_manual(values=viridis_pal(option = "D")(6))
#install.packages("pacman")
library(pacman)
library(dplyr)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
#install.packages("ggraph")
library(ggraph)
library(ggplot2)
library(plotly)
#install.packages("tm")
library(tm)
#install.packages("tidyquant")
library(tidyquant)
#install.packages(tidytext)
library(tidytext)
#install.packages("RTextTools")
#install.packages("RTextTools", repos = "http://r.findata.org")
#install.packages("RTextTools", dependencies=TRUE, repos='http://cran.rstudio.com/')
#install.packages('RTextTools',repos='http://cran.us.r-project.org')
library(SnowballC)
library(topicmodels)
#library(ggthemes)
#nstall.packages("waff")
#library(waff)
library(grid)
library(gridExtra)
library(dplyr)
pacman::p_load(tidyverse,tidytext,viridis,rvest,tm,wordcloud,SnowballC,tidyquant,ggridges,scales,highcharter,topicmodels)
n=507
#The reviews has 507 pages,thus n=507
urls <- paste0("https://www.glassdoor.co.in/Reviews/Northwestern-Mutual-Reviews-E2919_P",seq(2, n), ".htm?filter.defaultEmploymentStatuses=false&filter.defaultLocation=false")
urls<-c("https://www.glassdoor.co.in/Reviews/Northwestern-Mutual-Reviews-E2919.htm?filter.defaultEmploymentStatuses=false&filter.defaultLocation=false",urls)
html <- urls %>%
map_chr(~ read_html(.) %>% html_node(".hreview")%>%html_text())
