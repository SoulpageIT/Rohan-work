---
title: "GlassDoor Employee Rating Analysis of NMFinancial."
output: html_notebook
---

#About the Company

The Northwestern Mutual is an American financial services mutual organization based in Milwaukee. The financial security company provides consultation on wealth and asset income protection, education planning, retirement planning, investment advisory services, trust and private client services, estate planning and business planning. Its products include life insurance, disability income, and long-term care insurance; annuities; investments; and investment advisory products and services.[2] Northwestern Mutual ranked No. 104 in the 2018 Fortune 500 list of the largest United States corporations by total revenue.

Libraries
```{r}
#install.packages("pacman")
library(pacman)
library(dplyr)
library(viridis)
library(ggridges)
library(igraph)
library(highcharter)
#install.packages("ggraph")
library(ggraph)
library(ggplot2)
library(plotly)
#install.packages("tm")
library(tm)
#install.packages("tidyquant")
library(tidyquant)
#install.packages(tidytext)
library(tidytext)
#install.packages("RTextTools")
#install.packages("RTextTools", repos = "http://r.findata.org")
#install.packages("RTextTools", dependencies=TRUE, repos='http://cran.rstudio.com/')
#install.packages('RTextTools',repos='http://cran.us.r-project.org')
library(SnowballC)
library(topicmodels)
#library(ggthemes)
#nstall.packages("waff")
#library(waff)
library(grid)
library(gridExtra)
library(dplyr)

```


```{r}
pacman::p_load(tidyverse,tidytext,viridis,rvest,tm,wordcloud,SnowballC,tidyquant,ggridges,scales,highcharter,topicmodels)
```


```{r}
n=507
#The reviews has 507 pages,thus n=507
urls <- paste0("https://www.glassdoor.co.in/Reviews/Northwestern-Mutual-Reviews-E2919_P",seq(2, n), ".htm?filter.defaultEmploymentStatuses=false&filter.defaultLocation=false")
urls<-c("https://www.glassdoor.co.in/Reviews/Northwestern-Mutual-Reviews-E2919.htm?filter.defaultEmploymentStatuses=false&filter.defaultLocation=false",urls)
html <- urls %>%
    map_chr(~ read_html(.) %>% html_node(".hreview")%>%html_text())
html[[1]]
FCA_html<-html
```


```{r}
#Data-Preprocessing: removing '\n'
FCA_html<-gsub("\n","",FCA_html)
#remove all round brackets
FCA_html<-FCA_html%>%str_replace_all("\\(|\\)", "")
#remove all \\
FCA_html<-FCA_html%>%str_replace_all("\\\\", "")
#remove all non words and non numbers
#FCA_html<-FCA_html%>%str_replace_all("[^A-Za-z0-9]", "")
#remove all • 
FCA_html<-FCA_html%>%str_replace_all("\\•  ", "")
#remove all & 
FCA_html<-FCA_html%>%str_replace_all("\\ & ", "")
#remove all  non printable words
FCA_html<-FCA_html%>%str_replace_all("[^[:print:]]", "")
#remove all \
FCA_html<-FCA_html%>%str_replace_all(pattern = "\"", replacement = "")
#FCAindeed2<-FCAindeed2%>%stringi::stri_unescape_unicode()
# remove digits
#FCA_html%>%str_replace_all(pattern = "[[:digit:]]+", replacement = "")
#tm::removeNumbers(FCA_html)
#### pattern for dates
pattern ="\\(?\\d{4}\\)?[.-]? *\\d{2}[.-]? *[.-]?\\d{2}"
date=FCA_html%>%str_extract_all(pattern)
#FCA_html[[1]]%>%str_subset(pattern = "([0-9]{1,2})[- .]([a-zA-Z]+)[- .]([0-9]{4})")
#FCA_html[[1]]
#unlist(Date)
Date=as.Date(unlist(date))
#FCA_html_2=data_frame(Date=as.Date(unlist(date)),FCA_html)
```

Here we are Getting the count of all the type sentiments. 
```{r}
get_sentiments(lexicon = "nrc")%>%
    count(sentiment, sort = TRUE)
```

```{r}
GlassdoorPages <- data_frame(date=as.Date(unlist(date)),page = seq(1, n),
                      text = c(FCA_html))%>%arrange(desc(date))
#nrow(GlassdoorPages)
#write.csv(GlassdoorPages,"GlassdoorPages.csv",row.names = F)
```

```{r}
tidy_FCA <- GlassdoorPages %>%
    unnest_tokens(word, text) %>%
    add_count(date) %>%
    dplyr::rename(date_total = n)
#remove stop words
data("stop_words")
tidy_FCA <- tidy_FCA %>%
  anti_join(stop_words)
```

```{r}
stop_user=c("linklink","whatsappshar","auburn","twittershar","edit","delet","via","edit","delet","via","starstarstarstarstarwork","pdt","hill","ago",
            "facebookshar")
stop_user2=data_frame(word=stop_user)
tidy_FCA <- tidy_FCA %>%
  anti_join(stop_user2)
tidy_FCA%>%head()
```
```{r}
FCA_sentiment <- tidy_FCA %>%
    inner_join(get_sentiments("nrc"))
FCA_sentiment%>%head()
```

```{r}
theme_set(theme_bw())
#Alternatively
#FCA_sentiment%>%group_by(page, page_total, sentiment)%>%count()
FCA_sentiment %>%
    count(date, date_total, sentiment) %>%
    filter(sentiment %in% c("positive", "negative", 
                            "joy", "trust","fear","sadness"))%>%
    mutate(sentiment = as.factor(sentiment)) %>%
    #ggplot(aes(page, n / page_total, fill = sentiment)) +
     ggplot(aes(date, n / sum(n), fill = sentiment)) +
    geom_area(position = "identity", alpha = 0.5) +
    labs(y = "Relative frequency", x = "Year",
         title = "Sentiment analysis of NMFinancial Glassdoor Employee Reviews",
         subtitle = "Using the nrc lexicon")+theme_bw()+
 scale_fill_manual(values=viridis_pal(option = "D")(6))+
   scale_y_continuous(labels = scales::percent)

```

```{r}
FCA_sentiment %>%
    count(date, date_total, sentiment) %>%
    filter(sentiment %in% c("positive", "negative", 
                            "joy", "trust","fear","sadness"))%>%
    mutate(sentiment = as.factor(sentiment)) %>%
    #ggplot(aes(page, n / page_total, fill = sentiment)) +
     ggplot(aes(x=date,y= n / sum(n), fill = sentiment,height=n / sum(n),group=sentiment)) +
   geom_ridgeline_gradient() +
    labs(y = "Relative frequency", x = "Year",
         title = "Sentiment analysis of NMFinancial  Employee  Glassdoor Reviews",
         subtitle = "Using the nrc lexicon")+theme_bw()+
 scale_fill_viridis(discrete = TRUE, direction = -1) +
   scale_y_continuous(labels = scales::percent)
```
```{r}
FCA_sentiment %>%
    count(date, date_total, sentiment) %>%
  #  filter(sentiment %in% c("positive", "negative",  "joy", "trust","fear","sadness"))%>%
  mutate(sentiment = forcats::fct_lump(sentiment, 6))%>%
    #mutate(sentiment = as.factor(sentiment)) %>%
    ggplot(aes(date, n / date_total, fill = sentiment)) +
     #ggplot(aes(page, n / sum(n), fill = sentiment)) +
    geom_area(position = "identity", alpha = 0.5) +
    labs(y = "Relative frequency", x = "Year",
         title = "Sentiment analysis of NMFinancial  Employee Glassdoor Reviews",
         subtitle = "Using the nrc lexicon")+theme_bw()+
 scale_fill_manual(values=viridis_pal(option = "A")(7))+
   scale_y_continuous(labels = scales::percent)
```

```{r}
tidy_FCA %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(date) %>%
  summarize(average_sentiment = mean(score), words = n()) %>%
  #filter(words >= 10) %>%
  ggplot(aes(date, average_sentiment)) +
  geom_line() +
  geom_hline(color = "red", lty = 2, yintercept = 0) +
labs(y = "Average AFINN sentiment score", x = "Year",
         title = "Sentiment analysis of NMFinancial  Employee Glassdoor Reviews",
         subtitle = "Using the affin lexicon")+
  geom_smooth(method = "loess")
```

```{r}
ldat=tidy_FCA %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(date) %>%
  summarize(average_sentiment = mean(score), words = n())
highchart() %>% 
    hc_title(text = "Sentiment analysis of NMFinancial  Employee Glassdoor Reviews")%>%
  hc_add_series_times_values(ldat$date, ldat$average_sentiment, 
                             name = "Year",color="#440154FF")%>%
   hc_yAxis(title = list(text = "Average AFINN sentiment score"),labels = list(format = "{value}"), max = 4,min=-4,plotLines = list(
             list(label = list(text = ""),
                  color = "#35B779FF",
                  width = 2,
                  value = 0)))
```

```{r}
highchart(type = "stock") %>% 
  hc_title(text = "Sentiment analysis of NMFinancial  Employee Glassdoor Reviews") %>% 
  hc_subtitle(text = "") %>% 
  hc_tooltip(valueDecimals = 2) %>% 
  hc_add_series_times_values(ldat$date, ldat$average_sentiment,
                             name = "",color="#440154FF")%>% 
  hc_add_theme(hc_theme_gridlight())%>%
  hc_yAxis(title = list(text = "Average AFINN sentiment score"),labels = list(format = "{value}"), max = 4,min=-4,plotLines = list(
             list(label = list(text = ""),
                  color = "red",
                  width = 2,
                  value = 0)))
```


```{r}
tidy_FCA %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(date) %>%
  summarize(average_sentiment = mean(score), words = n()) %>%
 # filter(words >= 5) %>%
  ggplot(aes(date, average_sentiment)) +
  
   geom_line( )+
    theme_minimal()+
    geom_ridgeline_gradient(aes(y=0,height=average_sentiment,fill=average_sentiment),min_height=-3.5)+
    scale_fill_viridis(option="C",limit=c(-3.5,4))+
  
  #geom_line() +
  geom_hline(color = "red", lty = 2, yintercept = 0) +
labs(y = "Average AFINN sentiment score", x = "Page",
         title = "Sentiment analysis of NMFinancial  Employee Glassdoor Reviews",
         subtitle = "Using the affin lexicon")

```



```{r}
FCA_sentiment %>%
    count(sentiment, word) %>%
    filter(sentiment %in% c("positive", "negative", 
                            "joy", "trust","fear","sadness")) %>%
    group_by(sentiment) %>%
    top_n(10) %>%
    ungroup %>%
    mutate(word = reorder(word, n)) %>%
   mutate(sentiment = as.factor(sentiment))  %>%
    ggplot(aes(word, n, fill = sentiment)) +
    geom_bar(alpha = 0.8, show.legend = FALSE,stat = "identity") +
    coord_flip() +
    scale_y_continuous(expand = c(0,0)) +
    facet_wrap(~sentiment, scales = "free") +
   labs(y = "Total number of occurrences", x = "",
         title = "Sentiment analysis of NMFinancial  Employee Glassdoor Reviews",
         subtitle = "Using the nrc lexicon")+theme_bw()+
scale_fill_manual(values=viridis_pal(option = "D")(6))
```

#Plot without viridis package
```{r}
FCA_sentiment %>%
    count(date, date_total, sentiment) %>%
    filter(sentiment %in% c("positive", "negative", 
                            "joy", "trust","fear","sadness"))%>%
     mutate(sentiment = factor(sentiment, levels = c("negative",
                                                    "positive",
                                                    "joy", "trust","fear","sadness"))) %>%
    ggplot(aes(date, n / date_total, fill = sentiment)) +
    geom_area(position = "identity", alpha = 0.5) +
    labs(y = "Relative frequency", x = NULL,
         title = "Sentiment analysis NMFinancial  Employee Glassdoor Reviews",
         subtitle = "Using the nrc")+theme_bw()
```
Using bing Lexicon
```{r}
FCA_sentiment <- tidy_FCA %>%
    inner_join(get_sentiments("bing"))
FCA_sentiment %>%
    count(date, date_total, sentiment)%>%
    mutate(sentiment = as.factor(sentiment))%>%
    ggplot(aes(date, n / date_total, fill = sentiment)) +
    geom_area(position = "identity", alpha = 0.5) +
    labs(y = "Relative frequency", x = "Page",
         title = "Sentiment analysis NMFinancial  Employee Glassdoor Reviews",
         subtitle = "Using the nrc")+theme_bw()+
# scale_fill_manual(values=viridis_pal(option = "plasma")(2))+
   scale_y_continuous(labels = scales::percent)

```

```{r}
GlassdoorPages %>%
    unnest_tokens(word, text)%>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE)%>%
  spread(sentiment,n,fill=0)%>%
  mutate(sentiment = positive -negative)%>%
  ggplot(aes(x = sentiment)) +
geom_density(color = palette_light()[1], fill = palette_light()[1], alpha = 0.8) +
theme_tq()+xlim(c(-5,5))
```

```{r}
den=GlassdoorPages %>%
    unnest_tokens(word, text)%>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE)%>%
  spread(sentiment,n,fill=0)%>%
  mutate(sentiment = positive -negative)
#hchart(den,density(den$sentiment), type = "area", color = "#B71C1C", name = "Density")
hchart(density(den$sentiment), type = "area", color =viridis_pal()(1), name = "Sentiment")%>%
  hc_xAxis(min = -5, max =5)%>%
  hc_yAxis(title = list(text = "density"),labels = list(format = "{value}"))
```

```{r}
FCA_sentiment %>%
    count(sentiment, word) %>%
    group_by(sentiment) %>%
    top_n(15) %>%
    ungroup %>%
    mutate(word = reorder(word, n)) %>%
   mutate(sentiment = as.factor(sentiment))  %>%
    ggplot(aes(word, n, fill = sentiment)) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    coord_flip() +
    scale_y_continuous(expand = c(0,0)) +
    facet_wrap(~sentiment,scales="free") +
   labs(y = "Total number of occurrences", x = "",
         title = "Sentiment analysis of NMFinancial  Employee Glassdoor Reviews",
         subtitle = "Using the bing lexicon")+
#scale_fill_manual(values=viridis_pal(option = "D")(8))+
 scale_fill_viridis(end = 0.75, discrete=TRUE, direction = -1) +
        scale_x_discrete(expand=c(0.02,0)) +
        theme(strip.text=element_text(hjust=0)) +
  # change text into italics
        theme(strip.text = element_text(face = "italic")) +
  # strip horizontal  axis labels
        theme(axis.title.x=element_blank()) +
        theme(axis.ticks.x=element_blank()) +
        theme(axis.text.x=element_blank())+
  theme_minimal(base_size = 13)
```
The count most common positive and negative sentiment is displayed graphicaly below.
```{r}
bing_word_counts <-tidy_FCA %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts%>%spread(sentiment,n,fill = 0)%>%top_n(10)
```
```{r}
bing_word_counts%>%spread(sentiment,n,fill = 0)%>%top_n(-10)%>%head(10)
```
```{r}
bing_word_counts %>%
  filter(n > 3) %>%
  mutate(n = if_else(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(y="Contribution to sentiment",title="bing sentiments")+
#scale_fill_manual(values=viridis_pal(option = "D")(2))
 scale_fill_viridis(end = 0.85, discrete=TRUE, direction = 1)
```
Alternative way of scrapping from several web pages
```{r}
#FCA_html2<-FCA_html%>%str_replace_all("[[:xdigit:]]", "")
corpus = Corpus(VectorSource(FCA_html))
corpus = tm_map(corpus, tolower)
corpus<- tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removeWords,stop_user )
tdm <- TermDocumentMatrix(corpus,
                          control = list(removePunctuation = TRUE, 
                                      stopwords =  TRUE, 
                                      removeNumbers = TRUE, tolower = TRUE,
                                      PlainTextDocument=TRUE,
                                      stripWhitespace=TRUE, stemming = TRUE))
inspect(tdm)
```

```{r}
tidy(tdm)
```

```{r}
tdm = as.matrix(tdm)
```

```{r}
frequencies = DocumentTermMatrix(corpus)
frequencies
```

```{r}
findFreqTerms(frequencies, lowfreq=100)
```

```{r}
sparse = removeSparseTerms(frequencies, 0.995)
sparse
```

```{r}
findAssocs(frequencies, c("love","poor","flexible","horrible"), c(0.6,0.6,0.6,0.6))
```
#High Frequency Words
The most commonly words used in the reviews is plotted below.

#Word Cloud with Bing Lexicon
The most common positive and negative words are graphically depicted below.

```{r}
 GlassdoorPages %>%
    unnest_tokens(word, text)%>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
 reshape2::acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = viridis_pal(option = "D")(2),
                   max.words = 100)
```
#Word Cloud with nrc Lexicon
```{r}
GlassdoorPages %>%
    unnest_tokens(word, text)%>%
  inner_join(get_sentiments("nrc")) %>%
  count(word, sentiment, sort = TRUE)%>%
  spread( word,n,fill = 0)%>%head(5)
```
Among some of the words commonly used by reviewers to express positive,negative,joy or sadness is displayed in the word cloud below.
```{r}
GlassdoorPages %>%
    unnest_tokens(word, text)%>%
  inner_join(get_sentiments("nrc")) %>%
  count(word, sentiment, sort = TRUE)%>%
filter(sentiment %in% c("negative","positive","joy","sadness"))%>%
reshape2::acast(word ~ sentiment, value.var = "n", fill = 0)%>% 
comparison.cloud(colors = viridis_pal(option = "D")(4),
                  max.words = 200)
```

#Network Graph Visualization
```{r}
tidy_descr_ngrams=GlassdoorPages %>%
unnest_tokens(word, text, token = "ngrams", n = 2) %>%
separate(word, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)%>%
filter(!word1 %in% stop_user) %>%
filter(!word2 %in% stop_user)%>%
 mutate(word1 = removeNumbers(word1))%>%
mutate(word2 = removeNumbers(word2))
bigram_counts=tidy_descr_ngrams %>%
count(word1, word2, sort = TRUE)
bigram_graph =bigram_counts %>%
filter(n > 10) %>%
graph_from_data_frame()
set.seed(1)
a=grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = palette_light()[1], size = 5, alpha = 0.8) +
geom_node_text(aes(label = name), vjust = 1, hjust = 0.5) +
theme_void()
```
```{r}
data(stop_words)
tidy_descr<-GlassdoorPages %>%
unnest_tokens(word, text) %>% 
mutate(word=removeNumbers(word))%>% 
mutate(word_stem = wordStem(word)) %>%
anti_join(stop_words, by = "word") %>%
filter(!word_stem %in% stop_words$word) %>%
filter(!word_stem %in% stop_user) 
tidy_descr %>%
count(word_stem, sort = TRUE) %>%
filter(n > 30) %>%
ggplot(aes(x = reorder(word_stem, n), y = n)) +
geom_col(color = palette_light()[1], fill = palette_light()[1], alpha = 0.8) +
coord_flip() +
theme_tq() +
labs(x = "",
y = "count of most common words",titlt="Count of most common words")
```
```{r}
tidy_descr %>%
count(word_stem) %>%
mutate(word_stem = removeNumbers(word_stem)) %>%
with(wordcloud(word_stem, n, max.words = 100, colors = palette_light()))
```
```{r}
bigram_counts %>%
 mutate(word1 = removeNumbers(word1))%>%
mutate(word2 = removeNumbers(word2))%>% 
filter(n > 20) %>%
ggplot(aes(x = reorder(word1,-n), y = reorder(word2,-n), fill = n)) +
geom_tile(alpha = 0.8, color = "white") +
scale_fill_gradientn(colours = c(palette_light()[[1]], palette_light()[[2]])) +
coord_flip() +
theme_tq() +
theme(legend.position = "right") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
labs(x = "first word in pair",
y = "second word in pair")
```

```{r}
tidy_descr_ngrams=GlassdoorPages %>%
unnest_tokens(word, text, token = "ngrams", n = 2)%>%
separate(word, c("word1", "word2"), sep = " ")%>%
mutate(word1=removeNumbers(word1))%>% 
mutate(word1 = wordStem(word1))%>%
mutate(word2=removeNumbers(word2))%>% 
mutate(word2 = wordStem(word2))
tidy_descr_ngrams
```

```{r}
 tidy_FCA %>%
    inner_join(get_sentiments("bing"))%>%
  group_by(sentiment)%>%count()%>%
ggplot(aes(x = reorder(sentiment, n), y = n,fill=palette_light()[1])) +
geom_col(  alpha = 0.8,width = 0.5) +
coord_flip() +
theme_tq()+
labs(y="sentiments ",title="bing lexicon sentiment count" ,x="frequency")+
  theme(legend.position="none")+
#scale_fill_viridis(end = 0.85, discrete=TRUE, direction = 1,option = "D") 
#scale_fill_manual(values=viridis_pal(option = "A")(2))
scale_fill_tq()
```

```{r}
tidy_FCA %>%
    inner_join(get_sentiments("nrc"))%>%
  group_by(sentiment)%>%count()%>%
ggplot(aes(x = reorder(sentiment, n), y = n,fill=palette_light()[1])) +
geom_col(  alpha = 0.8) +
coord_flip() +
theme_tq()+
labs(y="sentiments ",title="nrc lexicon sentiment count" ,x="frequency")+
  theme(legend.position="none")+
#scale_fill_viridis(end = 0.85, discrete=TRUE, direction = 1,option = "D") 
#scale_fill_manual(values=viridis_pal(option = "A")(2))
scale_fill_tq()
```
#Topic Modelling

Topic modeling is a method for unsupervised classification of documents, by modeling each document as a mixture of topics and each topic as a mixture of words. Latent Dirichlet allocation(LDA) is a particularly popular method for fitting a topic model.

We can investigate what are the top five topics in the reviews by topic modelling.
```{r}
dtm_words_count<-tidy_descr %>%
mutate(word_stem = removeNumbers(word_stem)) %>%
count(date, word_stem, sort = TRUE) %>%
ungroup() %>%
filter(word_stem != "") %>%
# Casting a data frame to a DocumentTermMatrix, TermDocumentMatrix, or dfm  
cast_dtm(date, word_stem, n)
# set a seed so that the output of the model is predictable
dtm_lda<-LDA(dtm_words_count, k = 5, control = list(seed = 1234))
topics_beta<-tidy(dtm_lda, matrix = "beta")
```

```{r}
p1<-topics_beta %>%
filter(grepl("[a‐z]+", term)) %>% # extract alphabets a-z
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta) %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, color = factor(topic), fill = factor(topic))) +
geom_col(show.legend = FALSE, alpha = 0.8) +
scale_color_manual(values = palette_light()) +
scale_fill_manual(values = palette_light()) +
facet_wrap(~ topic, ncol = 5) +
coord_flip() +
theme_tq() +
labs(x = "",
y = "beta (~ occurrence in topics 1‐5)",
title = "The top 10 most characteristic words describe topic categories.")
```

```{r}
user_topic<-tidy(dtm_lda, matrix = "gamma") %>%
arrange(desc(gamma)) %>%
group_by(document) %>%
top_n(1, gamma)
```

```{r}
p2<-user_topic %>%
group_by(topic) %>%
top_n(10, gamma) %>%
ggplot(aes(x = reorder(document, -gamma), y = gamma, color = factor(topic))) +
facet_wrap(~ topic, scales = "free", ncol = 5) +
geom_point(show.legend = FALSE, size = 4, alpha = 0.8) +
scale_color_manual(values = palette_light()) +
scale_fill_manual(values = palette_light()) +
theme_tq() +
coord_flip() +
labs(x = "",
y = "gamma\n(~ affiliation with topics 1‐5)")

```

```{r}
p1
```
```{r}
p2
```
The dataframe can be cast into a document-term matrix (one-term-per-document-per-row) with the cast_dtm function in tidytext.The LDA function accepts input of this format.
```{r}
date_dtm<-GlassdoorPages %>%
mutate(year=year(date))%>%  
unnest_tokens(word, text) %>%
anti_join(stop_words)%>%
anti_join(stop_user2)  %>%
filter(!word %in% stop_user2) %>%
mutate(word=removeNumbers(word))%>% 
mutate(word = wordStem(word))%>%
filter(word != "") %>%
count(year, word, sort = TRUE) %>%
ungroup()%>%
cast_dtm(year, word, n)
 
```
We can now use topicmodels package to create a five topic LDA model.
```{r}
date_lda <- LDA(date_dtm, k = 5, control = list(seed = 1234))
```

```{r}
date_lda_td <- tidy(date_lda)
date_lda_td%>%head()

```
The β represent the probability that each term on a row belongs the topic on the row.
```{r}
top_terms <- date_lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  filter(grepl("[a‐z]+", term)) %>% # extract alphabets a-z
  arrange(topic, -beta)
top_terms%>%head()
```

```{r}
top_terms %>%mutate(topic=as.factor(topic))%>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = topic)) +
  geom_bar(stat = "identity",show.legend = FALSE, alpha = 0.8) +
  facet_wrap(~ topic, scales = "free") +
  theme(axis.text.x = element_text(size = 15, angle = 90, hjust = 1))+
  coord_flip()+
  scale_fill_viridis(end = 0.75, discrete=TRUE, direction = -1,option = "D")+
  labs(x = "",
y = "beta (occurrence in topics 1-5)",
title = "The top 10 most characteristic words describe topic categories.")
```
We treat each year beginning from 2008 as a separate document. Setting matrix = “gamma” returns a tidied version with one-document-per-topic-per-row. Now that we have these document classifications, we can see how well our unsupervised learning did at distinguishing the five topics. First we re-separate the document name into title and chapter:


```{r}
date_lda_gamma <- tidy(date_lda, matrix = "gamma")%>%mutate(topic=factor(topic))
date_lda_gamma
```

```{r}
ggplot(date_lda_gamma, aes(gamma, fill = topic)) +
  geom_histogram(bins = 30,binwidth=0.25) +
  facet_wrap(~ document, nrow = 2)+
  scale_fill_viridis(end = 0.75, discrete=TRUE, direction = -1)+theme_tq()

```

```{r}

```

